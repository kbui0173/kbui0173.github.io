[
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "From Fairways to Data Science",
    "section": "",
    "text": "Me and my mother celebrated my 16th birthday in The US, a few days after I landed in The States.\n\n\n\n\n\n\n\nMy first meeting with Miss Tiffany Elliot (my highschool academic advisor.).\n\n\n\n\nIf there was one thing I was certain about the moment I decided to convince my dad to let me go to the US alone at 15, it was that I wasn’t certain at all. Despite my father’s strong opposition to my dream, I managed to convince him to let me go. I told him, “I only live once. If I don’t get to pursue this dream in this life, then in which life can I?”\nI had zero clue why I wanted to do this, but there was an urge in me to do it: the feeling of doing something that excites me every morning and wears me out by bedtime. I loved the game, I loved competing, and I loved the challenge ahead of me. I looked up to guys on the PGA Tour, and my only thought was, “I want to live the life they live.”\nI didn’t care about the odds of becoming one of the top 100 guys out of 8 billion in this world. Well, that statistic isn’t quite right. To find \\(P(\\text{being top 100}) = 100/n\\), where \\(n\\) is the number of guys who tried. The odds were slim and pretty much against me. Most of my friends started as soon as they could walk. At the age of 14, I was considered late. More importantly, I knew I was not at the level of my friends, but I knew I wanted to beat them, and that’s all I cared about as a kid. I took every tournament personally; I wanted to beat everyone, and I wanted to beat the version of myself from yesterday. That was it, enough to let a 15 year old leave home to start a life in a new country."
  },
  {
    "objectID": "blog/posts/welcome/index.html#as-a-15-year-old-i-had-a-dream",
    "href": "blog/posts/welcome/index.html#as-a-15-year-old-i-had-a-dream",
    "title": "From Fairways to Data Science",
    "section": "",
    "text": "Me and my mother celebrated my 16th birthday in The US, a few days after I landed in The States.\n\n\n\n\n\n\n\nMy first meeting with Miss Tiffany Elliot (my highschool academic advisor.).\n\n\n\n\nIf there was one thing I was certain about the moment I decided to convince my dad to let me go to the US alone at 15, it was that I wasn’t certain at all. Despite my father’s strong opposition to my dream, I managed to convince him to let me go. I told him, “I only live once. If I don’t get to pursue this dream in this life, then in which life can I?”\nI had zero clue why I wanted to do this, but there was an urge in me to do it: the feeling of doing something that excites me every morning and wears me out by bedtime. I loved the game, I loved competing, and I loved the challenge ahead of me. I looked up to guys on the PGA Tour, and my only thought was, “I want to live the life they live.”\nI didn’t care about the odds of becoming one of the top 100 guys out of 8 billion in this world. Well, that statistic isn’t quite right. To find \\(P(\\text{being top 100}) = 100/n\\), where \\(n\\) is the number of guys who tried. The odds were slim and pretty much against me. Most of my friends started as soon as they could walk. At the age of 14, I was considered late. More importantly, I knew I was not at the level of my friends, but I knew I wanted to beat them, and that’s all I cared about as a kid. I took every tournament personally; I wanted to beat everyone, and I wanted to beat the version of myself from yesterday. That was it, enough to let a 15 year old leave home to start a life in a new country."
  },
  {
    "objectID": "blog/posts/welcome/index.html#and-just-like-my-dad-said-it-wasnt-easy",
    "href": "blog/posts/welcome/index.html#and-just-like-my-dad-said-it-wasnt-easy",
    "title": "From Fairways to Data Science",
    "section": "And just like my dad said it wasn’t easy…",
    "text": "And just like my dad said it wasn’t easy…\n\n\n\n\n\nA snapshot of me in a tournment.\n\n\n\n\n\n\n\nThis used to be my office!\n\n\n\n\nI was warned by my dad a thousand times that this journey wasn’t going to be easy. Well, I used to think, “No shit, Sherlock…” but he was right, and I was right too. It wasn’t easy, but the thing we both didn’t get right was just how hard it would be.\nLet me help you imagine my day-to-day routine. From Monday to Friday, it was repetitive:\n\n4:30 AM: Wake up and get ready.\n5:00 AM – 6:00 AM: In the gym, getting yelled at by the trainer.\n6:00 AM – 7:30 AM: Go home, get ready for school, and eat breakfast.\n8:00 AM – 1:00 PM: At school studying.\n1:00 PM – 1:30 PM: Grab a quick bite for lunch.\n2:00 PM – 7:00 PM: Golf training.\n7:00 PM – 10:00 PM: School work and wind down.\n\nYou may think now, Saturday and Sunday must be fun, right? Well, I would say it was tiring but fun at the same time. For half the weekends in a year, I played 18 holes carrying my 20-kilo golf bag in 35-degree Celsius weather. FYI, on average, I walked 15,000 to 20,000+ steps a day on the course. I repeated the same thing on Sunday to compete in tournaments.\nEvery other weekend, I was out there practicing. Every weekend, I had intrusive thoughts: I wish I could just sleep in and take a rest day. But I guess what kept me going was the love for the game and, more importantly, my coach’s harsh motivation: “If you’re not out there, there is some kid out there hitting 1,000 balls more than you. They are getting better than you, and you’re competing against them next weekend.” And that’s what kept me going."
  },
  {
    "objectID": "blog/posts/welcome/index.html#i-was-struggling-at-first-but-then-i-got-better",
    "href": "blog/posts/welcome/index.html#i-was-struggling-at-first-but-then-i-got-better",
    "title": "From Fairways to Data Science",
    "section": "I was struggling at first but then I got better…",
    "text": "I was struggling at first but then I got better…\nIn 2019, my scoring average was 83.44. That’s, on average, 11.44 shots over par (you want this to be as low as possible). If someone tells you they are playing competitively and they still shoot double digits over par… Well, that person is not good. And yep, that was me!\n\n\n\nMy scoring average was 83.44 in 2019.\n\n\nThen, 2020 was the year I was at my peak. I got my scoring average down by three strokes to 80.85 (that is roughly 8 shots over par). I started to play more competitively and I had a feeling like I could go out there and win this thing.\n\n\n\nMy scoring average was 80.85 in 2020.\n\n\nBut you guessed it right: COVID-19 hit. All of my tournaments got cancelled. My journey didn’t quite end there; I stayed and practiced for a bit more, but then I got called home by my father on the first rescue flight that flew 36 hours straight from the United States back home (Well we made a stop at Alaska).” Do you ever wonder what quarantine is like in Vietnam? Here’s a few snapshots I have left.\n\n\n\n\n\nThe longest flight of my life, 36 hours.\n\n\n\n\n\n\n\nThis is a millitary guest house for quarantine."
  },
  {
    "objectID": "blog/posts/welcome/index.html#i-was-wondering-what-comes-next-after-this",
    "href": "blog/posts/welcome/index.html#i-was-wondering-what-comes-next-after-this",
    "title": "From Fairways to Data Science",
    "section": "I was wondering what comes next after this…",
    "text": "I was wondering what comes next after this…\nAnd well you may ask- why don’t I continue? Unfortunately, my mother was diagnosed with cancer and I wanted to spend time studying closer to her during her last few years. She passed away in 2021 and fought a hard battle with cancer for 13 years. She was a true warrior and was my biggest motivation for whatever that I am doing today.\nAnd just like that, I stayed in Vietnam to take care of my mother and studied Finance and Economics at RMIT university of Vietnam. Well I didn’t quite graduate.. Economics and Finance were fun and all but something wasn’t right… It doesn’t challenge me. There’s no way I came from hustling with golf and then sitting at a table putting the right money into the right account and giving some pseudo explanations about a performance of a company that I have never worked in. I figured if I was sitting at a desk, I might as well be putting my brain into full work mode, not just doing something repetitively. And there was the AI boom, the year Chat-GPT came out, the word “Data Science” appeared more and more. Here is an illustration of the Google Search Keywords Analytics\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read data (skip 3 lines to get to actual data)\ndf = pd.read_csv(\"googlesearch.csv\", skiprows=3, names=['Month', 'Interest'])\ndf['Month'] = pd.to_datetime(df['Month'])\n\n# Create plot\nplt.figure(figsize=(9, 4))\nplt.plot(df['Month'], df['Interest'])\nplt.axvline(pd.to_datetime('2022-01-01'), color='red', linestyle='--', linewidth=2, label='2022: AI Boom Begins')\nplt.title('Google Search Interest: \"Data Science\" (2004-2025)')\nplt.xlabel('Year')\nplt.ylabel('Search Interest (0-100)')\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nI didn’t know what this major was or at least what was inside of it but I guessed, if there’s lots of people on that boat, the boat might be heading somewhere nice so I figured I might as well jump on it. And I wanted to try Sydney out, I have heard that the weather is nice like Los Angeles and people were “friendlier” than the United States. So, here I am, a Data Science Major studying at the University of Sydney. Clearly, I was unaware and given no knowledge of the maths this was involving. I knew there were more numbers and equations involved but I figured I was good at balancing the chemistry equations and I could solve some algebra equations back in high-school so I’ll give it a shot. And the first most maths heavy unit I did at USYD was QBUS1040 (Introduction to Business Analytics). Don’t let the name fool you; while it sounds easy, we actually covered advanced topics like regularization, multi-objective least squares, and constrained optimization using Lagrange multipliers. And yes, I studied all the calculus that I have missed by playing way too much golf in Highschool with little care on my academics. And yes, if there was one thing I could go back and change about my High School years were to focus on my academics at the same time as well."
  },
  {
    "objectID": "blog/posts/welcome/index.html#the-memories-that-i-have-left",
    "href": "blog/posts/welcome/index.html#the-memories-that-i-have-left",
    "title": "From Fairways to Data Science",
    "section": "The memories that I have left…",
    "text": "The memories that I have left…\nI remember the sandwich my coach gave me at the end of every tournament before our dinner feast. I remember the laughs we shared on the course, knowing we had all played terribly and there was nothing left to do but have fun and enjoy the expensive entry fee. At least the course was nice, right? I started considering tournaments as simply cheaper green fees with better hospitality at exclusive courses, rounds that would normally cost twice as much if I weren’t playing in a junior tournament. The fun part of this journey wasn’t just the opportunity to play competitive golf; it was the friendships I made along the way. I came to the US alone at 14. I missed home, but I found brothers in Danny, Kenny, Ryan, Lubo, Mike, and Jamie. To this day, I remember the 5 AM workouts we had to do because it was the only time our trainer was available before school. We were all grumpy and, as 14-year-olds, half-asleep. The workouts happened in pure silence. I remember some of us complaining, but the only response was, “Shut up and get it done.” This wasn’t out of hate; we were all going through the same pain, and we believed being a man meant quitting the whining and working through it. Every time I heard “shut up,” it was motivation, not an insult. Nothing felt as bad as waking up at 4:30 AM to train, but nothing felt as good as completing the workout and knowing I had achieved a small win for the day."
  },
  {
    "objectID": "blog/posts/welcome/index.html#after-all-the-dream-wasnt-completed-but-the-lessons-it-gave-me-were-valuables-to-complete-my-other-dreams.",
    "href": "blog/posts/welcome/index.html#after-all-the-dream-wasnt-completed-but-the-lessons-it-gave-me-were-valuables-to-complete-my-other-dreams.",
    "title": "From Fairways to Data Science",
    "section": "After all, the dream wasn’t completed but the lessons it gave me were valuables to complete my other dreams.",
    "text": "After all, the dream wasn’t completed but the lessons it gave me were valuables to complete my other dreams.\nJust like in golf, no matter how hard it is, how much you hate it, or whether you don’t know if you are going to make it or not, showing up is what you have to do. At times, I really have no clue what the professors are talking about in a lecture, but I just show up, attending every lecture I possibly can, even when I don’t understand and have to discuss it with ‘Professor GPT,’ Claude, or even Gemini (who has done a fantastic job too). Just show up, trust the process, and have patience with yourself."
  },
  {
    "objectID": "blog/posts/welcome/index.html#well-and-why-would-someone-reading-it-until-here-should-consider-giving-me-an-opportunity-at-the-role",
    "href": "blog/posts/welcome/index.html#well-and-why-would-someone-reading-it-until-here-should-consider-giving-me-an-opportunity-at-the-role",
    "title": "From Fairways to Data Science",
    "section": "Well, and why would someone reading it until here should consider giving me an opportunity at the role?",
    "text": "Well, and why would someone reading it until here should consider giving me an opportunity at the role?\nSo, why should you consider giving me an opportunity? I may not be the sort of math prodigy who wins Olympiads, but I am the person who sticks around, fights to understand difficult concepts, and finds solutions. I show up every morning, even when the work is stressful or the path isn’t clear. I believe this combination of curiosity, resilience, and adaptability makes me a valuable addition to your team."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Khanh N. (Alex) Bui",
    "section": "",
    "text": "Hi there, this is me!\n\n\nHello, my name is Khanh N. Bui and I’m also known as Alex.\n(Fun fact: I named my self after the Lion from Madagascars!).\nI’m a Data Science major at The University of Sydney, currently in my third year and considering pursuing Honours in Data Science.\nQuickly navigate my page using:\n\nProjects Blog Posts Resume"
  },
  {
    "objectID": "about/index.html#about-me",
    "href": "about/index.html#about-me",
    "title": "Khanh N. (Alex) Bui",
    "section": "",
    "text": "Hi there, this is me!\n\n\nHello, my name is Khanh N. Bui and I’m also known as Alex.\n(Fun fact: I named my self after the Lion from Madagascars!).\nI’m a Data Science major at The University of Sydney, currently in my third year and considering pursuing Honours in Data Science.\nQuickly navigate my page using:\n\nProjects Blog Posts Resume"
  },
  {
    "objectID": "about/index.html#academic-interests",
    "href": "about/index.html#academic-interests",
    "title": "Khanh N. (Alex) Bui",
    "section": "Academic Interests",
    "text": "Academic Interests\nMy academic interest lies at the intersection of Time Series Analysis and Financial Econometrics, specifically where we can leverage Bayesian Approximations to handle uncertainty. My goal is to pursue a PhD and advance these fields.\nAt my core, I am a Bayesian apologist. It’s not just about the computational utility; it’s the philosophy of updating beliefs that resonates with me. I often find myself teasing my colleagues: “Do frequentists really think they’re going to live long enough to verify those long-run results?” Give me a posterior distribution over a p-value any day.\nI have done some projects regarding these topics. You can check them out here!"
  },
  {
    "objectID": "about/index.html#beyond-academia",
    "href": "about/index.html#beyond-academia",
    "title": "Khanh N. (Alex) Bui",
    "section": "Beyond Academia",
    "text": "Beyond Academia\n\nMy dream was to become a Professional Golfer!\nOutside of the academic world, my heart belongs to the golf course. I’ve been playing since I was 10 years old, and for a long time, the goal wasn’t just to play, but to go pro. I took my training seriously, competing at a high level and even cracking the top 10 in a junior tournament in Orlando, Florida. Today, I play for the love of the game, but the mental toughness required to compete is a skill I carry into everything I do.\nWant to know more? I wrote about my golf journey here!\n\n\n\n\n\nHole-in-one with my PW, the best thing about it was that my father was right there too!\n\n\n\n\n\n\n\nI was fortunate to be coached by the same trainer (Dr. Brendan McLaughlin) of Justin Rose (when he was World’s #1.)\n\n\n\n\n\n\n\nThis is my back-swing. I’m not a professional golfer, but I’m trying my best.\n\n\n\n\n\n\n\n7th place in HJGT Tournament - Orlando, Florida\n\n\nView Official Tournament Results\n\n\n\n\nMy camera is technically my best friend\nWhen I’m not on the golf course, you’ll usually find me with a camera in hand. This passion was born from a quiet realization one New Year’s Eve: I scrolled through my camera roll looking for memories to hold onto, only to find it nearly empty. It felt like a whole year of beautiful, small moments had simply slipped through my fingers.\nThat night became a turning point. I promised myself I would never let life pass by unrecorded again. Lately, I’ve fallen in love with seeing the world through my Fujifilm X-S20; its film simulations give every shot a sense of timelessness and warmth. Below are some of the fleeting moments I’ve been lucky enough to keep.\n\n\n\n\n\nA view of CBD from another side\n\n\n\n\n\n\n\nObservatory Hill\n\n\n\n\n\n\n\nObservatory Hill\n\n\n\n\n\n\n\nSomewhere under the Bridge"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Exhaustive ARIMA Modeling for CPI Forecasting\n\n\n\n\n\n\nTime-Series\n\n\nQuantitative Finance\n\n\n\nExhaustive ARIMA model search for Australian CPI forecasting using expanding window cross-validation\n\n\n\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRegime-Switching Volatility Forecasting\n\n\n\n\n\n\nFinance\n\n\nTime Series\n\n\nVolatility Modeling\n\n\n\nCombining Hidden Markov Models with GARCH to forecast ASX 200 volatility across market regimes\n\n\n\n\n\nDec 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProject 3\n\n\n\n\n\n\ncategory\n\n\n\nDescription of project 3\n\n\n\n\n\nDec 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProject 4\n\n\n\n\n\n\ncategory\n\n\n\nDescription of project 4\n\n\n\n\n\nDec 25, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project-3/index.html",
    "href": "projects/project-3/index.html",
    "title": "Project 3",
    "section": "",
    "text": "Project 3 details coming soon…"
  },
  {
    "objectID": "projects/project-3/index.html#overview",
    "href": "projects/project-3/index.html#overview",
    "title": "Project 3",
    "section": "",
    "text": "Project 3 details coming soon…"
  },
  {
    "objectID": "projects/project-4/index.html",
    "href": "projects/project-4/index.html",
    "title": "Project 4",
    "section": "",
    "text": "Project 4 details coming soon…"
  },
  {
    "objectID": "projects/project-4/index.html#overview",
    "href": "projects/project-4/index.html#overview",
    "title": "Project 4",
    "section": "",
    "text": "Project 4 details coming soon…"
  },
  {
    "objectID": "projects/project-2/index.html",
    "href": "projects/project-2/index.html",
    "title": "Regime-Switching Volatility Forecasting",
    "section": "",
    "text": "To produce realistic volatility forecasts, I aim to combine two statistical concepts: Regime Detection (HMM) and Volatility Clustering (GARCH).\n\n\nThe Intuition: Financial markets have different “moods.” In some periods, the market is calm and rational; in others, it is panicked and irrational. We cannot directly observe these “market conditions”—we can only observe the returns.\nHow HMM helps: The Hidden Markov Model treats these conditions as unobserved regimes (e.g., Calm vs. Crisis). By analyzing the pattern of returns, the model infers the “Hidden State” of the market.\n\nDetects Shifts: It identifies when the market switches from a low-risk environment to a high-risk crash.\nEstimates Probability: Instead of a simple yes/no, it outputs a probability (e.g., “There is an 85% chance we are currently in a Crisis regime”).\n\n\n\n\nThe Intuition: Volatility is “sticky.” If the market was wild yesterday, it is likely to be wild today. Standard statistical models miss this, but GARCH (Generalized Autoregressive Conditional Heteroskedasticity) is designed specifically to capture it.\nThe Mechanics: The GARCH(1,1) model forecasts volatility (\\(h_t\\)) based on its own history:\n\\[\nh_t = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta h_{t-1}\n\\]\n\n\\(\\omega\\) (The Baseline): The long-term average volatility level.\n\\(\\alpha\\) (The Reaction): How strongly the market reacts to yesterday’s shock (“Did breaking news hit the market?”).\n\\(\\beta\\) (The Memory): How long that shock persists (“Is the panic fading away, or is it sticking around?”).\n\n\n\n\n\nStandard GARCH models have a major flaw: they assume the market’s behavior (the \\(\\alpha\\) and \\(\\beta\\) parameters) never changes. This is unrealistic—a market crash behaves fundamentally differently than a slow summer rally.\nThe Solution: Markov-Switching GARCH\nBy combining HMM and GARCH, we create a model that adapts to the environment:\n\nMultiple Engines: We fit two separate GARCH models. One captures the dynamics of a “Calm” market, and the other captures a “Turbulent” market.\nDynamic Switching: The model uses the HMM probabilities to weighted-switch between these two engines in real-time.\n\nResult: A forecast that not only predicts volatility but also adapts its rules when the market shifts gears."
  },
  {
    "objectID": "projects/project-2/index.html#overview",
    "href": "projects/project-2/index.html#overview",
    "title": "Regime-Switching Volatility Forecasting",
    "section": "",
    "text": "To produce realistic volatility forecasts, I aim to combine two statistical concepts: Regime Detection (HMM) and Volatility Clustering (GARCH).\n\n\nThe Intuition: Financial markets have different “moods.” In some periods, the market is calm and rational; in others, it is panicked and irrational. We cannot directly observe these “market conditions”—we can only observe the returns.\nHow HMM helps: The Hidden Markov Model treats these conditions as unobserved regimes (e.g., Calm vs. Crisis). By analyzing the pattern of returns, the model infers the “Hidden State” of the market.\n\nDetects Shifts: It identifies when the market switches from a low-risk environment to a high-risk crash.\nEstimates Probability: Instead of a simple yes/no, it outputs a probability (e.g., “There is an 85% chance we are currently in a Crisis regime”).\n\n\n\n\nThe Intuition: Volatility is “sticky.” If the market was wild yesterday, it is likely to be wild today. Standard statistical models miss this, but GARCH (Generalized Autoregressive Conditional Heteroskedasticity) is designed specifically to capture it.\nThe Mechanics: The GARCH(1,1) model forecasts volatility (\\(h_t\\)) based on its own history:\n\\[\nh_t = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta h_{t-1}\n\\]\n\n\\(\\omega\\) (The Baseline): The long-term average volatility level.\n\\(\\alpha\\) (The Reaction): How strongly the market reacts to yesterday’s shock (“Did breaking news hit the market?”).\n\\(\\beta\\) (The Memory): How long that shock persists (“Is the panic fading away, or is it sticking around?”).\n\n\n\n\n\nStandard GARCH models have a major flaw: they assume the market’s behavior (the \\(\\alpha\\) and \\(\\beta\\) parameters) never changes. This is unrealistic—a market crash behaves fundamentally differently than a slow summer rally.\nThe Solution: Markov-Switching GARCH\nBy combining HMM and GARCH, we create a model that adapts to the environment:\n\nMultiple Engines: We fit two separate GARCH models. One captures the dynamics of a “Calm” market, and the other captures a “Turbulent” market.\nDynamic Switching: The model uses the HMM probabilities to weighted-switch between these two engines in real-time.\n\nResult: A forecast that not only predicts volatility but also adapts its rules when the market shifts gears."
  },
  {
    "objectID": "projects/project-2/index.html#library-needed",
    "href": "projects/project-2/index.html#library-needed",
    "title": "Regime-Switching Volatility Forecasting",
    "section": "Library needed",
    "text": "Library needed\nTo successfully run this project, we will use the following library:\n\n# Load all required libraries\nlibrary(knitr)\nlibrary(quantmod)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(moments)\nlibrary(tseries)\nlibrary(MSGARCH)\nlibrary(gridExtra)\nlibrary(zoo)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(doParallel)\nlibrary(foreach)\nlibrary(kableExtra)"
  },
  {
    "objectID": "projects/project-2/index.html#data-preprocessing",
    "href": "projects/project-2/index.html#data-preprocessing",
    "title": "Regime-Switching Volatility Forecasting",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nFetching Index Prices\nFirst, we are going to fetch stock data for the ASX200 (^AORD) index from 2000 to the present.\n\n\nFetching Index Prices\n# 1. Configuration\nstart_date &lt;- \"2000-01-01\"\n\n# ASX200 (^AXJO)\nticker_map &lt;- list(\n  \"^AXJO\" = \"ASX200\"\n)\n\n# 3. Loop, Fetch, and Save \nfor (symbol in names(ticker_map)) {\n  data &lt;- getSymbols(symbol, src = \"yahoo\", from = start_date, auto.assign = FALSE)\n  adj_data &lt;- Ad(data)\n  names(adj_data) &lt;- paste0(ticker_map[[symbol]], \".Adjusted\")\n  write.zoo(adj_data, file = paste0(ticker_map[[symbol]], \".csv\"), sep = \",\", col.names = TRUE, index.name = \"Date\")\n}\n\n\nAbove, you may see how I use the library quantmod to fetch these index data and notice that I only use Adjusted Closing Price via Ad(data)\nLet’s see what the data we have fetched looks like.\n\nASX200\n\n\nCreate nice HTML for ASX200\n# Read the ASX200 data\nasx_data &lt;- read.csv(\"ASX200.csv\")\n\nDT::datatable(\n  asx_data,\n  options = list(\n    pageLength = nrow(asx_data),\n    scrollY = 200,\n    paging = FALSE,\n    searching = FALSE,\n    info = FALSE,\n    dom = 't',\n    columnDefs = list(\n      list(className = 'dt-center', targets = \"_all\")\n    )\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: bottom; text-align: center;',\n    'Scrollable: All ASX200 Rows'\n  )\n)\n\n\n\n\n\n\n\n\n\nDealing with NAs\n\n\nPercentage NAs in ASX200 dataset\n# Calculate % NAs in .Adjusted column\nasx_na &lt;- round(sum(is.na(asx_data$ASX200.Adjusted)) / length(asx_data$ASX200.Adjusted) * 100, 2)\n\n# Create table for display\nall_na_pct &lt;- data.frame(\n  Dataset = \"ASX200\",\n  NA_Percentage = asx_na\n)\n\nDT::datatable(\n  all_na_pct,\n  options = list(\n    dom = 't',\n    paging = FALSE,\n    columnDefs = list(list(className = 'dt-center', targets = \"_all\"))\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: bottom; text-align: center;',\n    '% NAs in ASX200 and S&P500 Datasets'\n  )\n)\n\n\n\n\n\n\nWe have identified there are a small portion of the data are missing. After some careful investigation, I have indentfied the reason behind this, let’s take a look at this example:\n\n\nInvestigating of Misaligned date.\n# Convert Date column to a proper Date object\nasx_data$Date &lt;- as.Date(asx_data$Date)\n\n# Add a 'Day_of_Week' column\nasx_data$Day_of_Week &lt;- weekdays(asx_data$Date)\n\n# We look at the first 10 rows where the shift is most obvious\nsubset_view &lt;- asx_data[1:6, c(\"Date\", \"ASX200.Adjusted\", \"Day_of_Week\")]\n\n# Display the subset_view as an HTML table\nDT::datatable(\n  subset_view,\n  options = list(\n    dom = 't',\n    paging = FALSE,\n    columnDefs = list(list(className = 'dt-center', targets = \"_all\"))\n  ),\n  rownames = FALSE,\n  caption = htmltools::tags$caption(\n    style = 'caption-side: bottom; text-align: center;',\n    'First Few Rows of ASX200 Dataset (with Day of Week)'\n  )\n)\n\n\n\n\n\n\n\nThe “Sunday” Error: We see a price for Sunday, Jan 9. The ASX is closed on Sundays. This is actually Monday’s price, shifted back by one day.\nThe “Missing Friday”: We see Thursday (Jan 6) followed immediately by Sunday (Jan 9). Friday Jan 7 is missing. The price 3023.40 listed on Thursday is actually the price for that missing Friday.\n\nThe solution: shift the ASX dates by +1 and we are going to check if Saturday and Sunday are still in the dataset. However, there are more than just that. I will explain each step clearly after this code chunk.\n\n\nFixes misaligned ASX dates by shifting ‘bad’ weeks and removing weekends\n# The Fix\nasx_fixed &lt;- asx_data %&gt;%\n  mutate(\n    Week_ID = paste(isoyear(Date), isoweek(Date), sep = \"-\"),\n    Day_Num = wday(Date, week_start = 1)\n  ) %&gt;%\n  group_by(Week_ID) %&gt;%\n  mutate(\n    Needs_Shift = any(Day_Num == 7)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Date_Corrected = if_else(Needs_Shift, Date + 1, Date)\n  ) %&gt;%\n  filter(wday(Date_Corrected, week_start = 1) &lt;= 5) %&gt;%\n  select(Date = Date_Corrected, ASX200.Adjusted)\n\nwrite.csv(asx_fixed, \"ASX200_Fixed.csv\", row.names = FALSE)\n\n\nNow let me explain what does the code do:\n1.Grouping by Week (Week_ID)\n\nmutate(\n  Week_ID = paste(isoyear(Date), isoweek(Date), sep = \"-\"),\n  Day_Num = wday(Date, week_start = 1)\n) %&gt;%\ngroup_by(Week_ID)\n\n\nLogic: The timezone error typically affects contiguous blocks of data. Instead of analyzing day-by-day, I group the data into ISO Weeks (Monday to Sunday). This allows the algorithm to assess the integrity of an entire trading week at once.\n\n2. Error Detection (Needs_Shift)\n\nmutate(\n  Needs_Shift = any(Day_Num == 7)\n)\n\n\nThe script scans each Week_ID for the presence of a Sunday (Day 7).\nThe ASX does not trade on Sundays. Therefore, if a week contains a “Sunday” timestamp, it confirms that the entire week’s data has been incorrectly shifted backward (e.g., Monday’s data is labeled as Sunday, Friday’s data is labeled as Thursday).\n\n3. Conditional Repair (if_else)\n\nmutate(\n  Date_Corrected = if_else(Needs_Shift, Date + 1, Date)\n)\n\n\nIf Needs_Shift is TRUE: The code adds +1 Day to every date in that week. This restores Sunday \\(\\rightarrow\\) Monday and Thursday \\(\\rightarrow\\) Friday.\nIf Needs_Shift is FALSE: The algorithm preserves the original dates, preventing the corruption of already-correct data (e.g., Good Friday holidays).\n\n4. Artifact Removal (filter)\n\nfilter(wday(Date_Corrected, week_start = 1) &lt;= 5)\n\n\nIn the misaligned weeks, the row originally labeled “Friday” (which was actually garbage or duplicate data) shifts to “Saturday” after the correction. This step strictly filters the final dataset to keep only Monday through Friday, automatically removing these artifacts."
  },
  {
    "objectID": "projects/project-2/index.html#exploratory-data-analyis",
    "href": "projects/project-2/index.html#exploratory-data-analyis",
    "title": "Regime-Switching Volatility Forecasting",
    "section": "Exploratory Data Analyis",
    "text": "Exploratory Data Analyis\n\nTime Series Chart\nThere is absolutely nothing more fascinating than being able to create a time-series chart after a while of wranling with the messy data. Here I present you the time series for our two indices.\n\n\nTime series chart for ASX200\n# Load necessary libraries\n\n# Read data\nasx_df &lt;- read_csv(\"ASX200_Fixed.csv\")\n\n# Ensure Date is Date type\nasx_df$Date &lt;- as.Date(asx_df$Date)\n\n# Plot time series\nggplot(asx_df, aes(x = Date, y = ASX200.Adjusted)) +\n  geom_line(color = \"red\", linewidth = 0.8) +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"2 years\") +\n  labs(\n    title = \"Time Series of ASX 200\",\n    x = \"Year\",\n    y = \"Adjusted Closing Price\"\n  ) +\n  theme_classic() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nComparing the raw prices of these indices is like comparing apples to oranges. To get a clearer picture of actual growth, I re-indexed both markets to a starting value of 100. This makes it much easier to track their relative performance side-by-side.\n\n\nPlot Re-indexed Growth (Base = 100)\nasx_base &lt;- asx_df$ASX200.Adjusted[1]\n\n# Create the indexed price column\nasx_df &lt;- asx_df %&gt;%\n  mutate(Indexed_Price = (ASX200.Adjusted / asx_base) * 100)\n\n# Plot ASX 200 growth\nggplot(asx_df, aes(x = Date, y = Indexed_Price)) +\n  geom_line(color = \"red\", linewidth = 0.8) +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"2 years\") +\n  labs(\n    title = \"Relative Growth: ASX 200\",\n    subtitle = \"Re-indexed to Base 100 (Jan 2000)\",\n    x = \"Year\",\n    y = \"Growth (Base = 100)\"\n  ) +\n  theme_classic() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\nLog Returns\nAttempting to model raw price levels is often futile because asset prices typically follow a random walk. However, volatility—the ‘risk’ of the market—is highly predictable because it clusters over time. To isolate this volatility signal and ensure our data is stationary (a requirement for GARCH), the first step is to transform our prices into log-returns.\nThe formula for log-returns is:\n\\[\nr_t = 100 \\times \\left(\\log P_t - \\log P_{t-1}\\right)\n\\]\nwhere \\(P_t\\) is the price at time \\(t\\).\n\n\nCalculate Log Returns\nasx_df &lt;- asx_df %&gt;%\n  mutate(Return = c(NA, diff(log(ASX200.Adjusted))) * 100) %&gt;%\n  na.omit() # Remove the first NA row\n\n\n\n\nLog Returns Visualization\nHere are the time series plots of daily log returns for each index:\n\nASX 200 Log Returns\n\n\nASX 200 Log Returns Time Series\nggplot(asx_df, aes(x = Date, y = Return)) +\n  geom_line(color = \"red\", linewidth = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\", alpha = 0.5) +\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"2 years\") +\n  labs(\n    title = \"ASX 200 Daily Log Returns\",\n    x = \"Year\",\n    y = \"Log Return (%)\"\n  ) +\n  theme_classic() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Analysis\nTo assess the distributional properties of our log returns and check for normality, here are histograms and Q-Q plots for the ASX 200:\n\nASX 200 Distributions\n\n\nASX 200 Histogram and Q-Q Plot\n# Create a 1x2 layout for histogram and Q-Q plot\npar(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n\n# Histogram\nhist(asx_df$Return,\n     main = \"ASX 200 Log Returns Distribution\",\n     xlab = \"Log Return (%)\",\n     col = \"lightcoral\",\n     border = \"white\",\n     breaks = 50,\n     freq = FALSE)\n\n# Add normal density curve\nx_seq &lt;- seq(min(asx_df$Return, na.rm = TRUE), max(asx_df$Return, na.rm = TRUE), length = 100)\nlines(x_seq, dnorm(x_seq, mean = mean(asx_df$Return, na.rm = TRUE),\n                   sd = sd(asx_df$Return, na.rm = TRUE)),\n      col = \"darkred\", lwd = 2)\n\n# Q-Q Plot\nqqnorm(asx_df$Return, main = \"ASX 200 Q-Q Plot\", col = \"red\", pch = 16)\nqqline(asx_df$Return, col = \"darkred\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics\nThe EDA would never be completed without seeing some important descriptive statistics:\n\n\nCompute descriptive statistics\nget_stats &lt;- function(x) {\n  list(\n    Mean = mean(x),\n    SD = sd(x),\n    Min = min(x),\n    Max = max(x),\n    Skewness = skewness(x),\n    Kurtosis = kurtosis(x), # Normal = 3\n    ADF_p_value = adf.test(x)$p.value\n  )\n}\n\nstats_asx &lt;- get_stats(asx_df$Return)\n\nstats_table &lt;- data.frame(\n  ASX_200 = unlist(stats_asx)\n)\n\n# Print nicely\nknitr::kable(stats_table, digits = 3, caption = \"Descriptive Statistics of ASX 200 Daily Log-Returns\", align = 'c')\n\n\n\nDescriptive Statistics of ASX 200 Daily Log-Returns\n\n\n\nASX_200\n\n\n\n\nMean\n0.015\n\n\nSD\n0.991\n\n\nMin\n-10.203\n\n\nMax\n6.766\n\n\nSkewness\n-0.723\n\n\nKurtosis\n11.434\n\n\nADF_p_value\n0.010\n\n\n\n\n\nHere are the interpretations of what those numbers mean:\n1. Stationarity Confirmed (ADF Test)\n\nThe ADF test yielded a p-value of 0.01 (\\(p &lt; 0.05\\)).\nNull Hypothesis Rejected: We reject the assumption of non-stationarity.\nLog-returns are confirmed to be stationary, satisfying the prerequisite for GARCH modeling.\n\n2. Volatility & Risk Profile\n\nVolatility Level: The ASX 200 exhibits a standard deviation of \\(0.991\\), indicating the level of daily volatility.\nExtreme Shock Sensitivity: The market shows significant tail risk, with a maximum single-day drawdown of -10.20%, suggesting the potential for sharp corrections.\n\n3. Evidence of “Fat Tails” (Kurtosis)\n\nThe ASX 200 exhibits extreme excess kurtosis (\\(11.43\\)), far exceeding the benchmark of \\(3.0\\) for a Normal Distribution.\nThis indicates a Leptokurtic distribution, where probability mass is concentrated in the center and the tails.\n\n\n\n\n\n\n\n\n\n\n\nPractically, this proves that “black swan” events occur far more frequently than a standard Gaussian model would predict, providing strong statistical justification for the use of MS-GARCH to capture these regime-shifting tail risks.\n\n4. Asymmetry (Skewness)\n\nBoth markets display negative skewness (\\(-0.723\\) for ASX and \\(-0.349\\) for S&P 500).\nThis asymmetry indicates that the distribution has a longer left tail, reflecting the financial reality that volatility feedback is stronger during downturns.\nIn simple terms, markets tend to crash faster and deeper than they rally—a phenomenon known as the “leverage effect” that simple symmetric models often fail to capture.\n\n\n\nSummary\nAll of that long writing basically mean…\n\nOur data testing confirms that returns are not Normally distributed or symmetric.\nWe observe a distinct negative skew, meaning ‘panic selling’ creates larger shocks than ‘panic buying.’\nTo model this reality, we will use the Skewed Student-t Distribution, which can account for the heavier losses found in the left tail."
  },
  {
    "objectID": "projects/project-2/index.html#modelling",
    "href": "projects/project-2/index.html#modelling",
    "title": "Regime-Switching Volatility Forecasting",
    "section": "Modelling",
    "text": "Modelling\nNow comes the fun part, we wil start to do some modelling. This step always gets me excited the most, I hope you do too!\n\nModel Specification Decisions\nBefore fitting the final model, we need to make three key choices:\n\nDefining the Regimes (\\(K\\)): Does the market switch between two states (e.g., Calm vs. Crisis), or are there more? We will test models with 1, 2, and 3 regimes.\nSelecting the Distribution: Since financial data is rarely Normal, we need to choose a distribution (like the Skewed Student-t) that handles “fat tails” effectively.\nChoosing the Engine: Does the market react the same way to good and bad news? We will test the standard sGARCH against the asymmetric gjrGARCH to see if we need to account for the “leverage effect.”\n\n\n\nFinding the Best GARCH Specification (Results Pre-computed)\n# 1. Define the Candidate Combinations\n# We test 3 dimensions: Volatility Model, Distribution, and Regimes\ncandidates &lt;- expand.grid(\n  Model = c(\"sGARCH\", \"gjrGARCH\"),\n  Dist  = c(\"norm\", \"sstd\"),\n  K     = c(1, 2),\n  stringsAsFactors = FALSE\n)\n\n# 2. Function to Fit and Extract AIC\nrun_bakeoff &lt;- function(data_vector, candidates) {\n  results &lt;- list()\n\n  for(i in 1:nrow(candidates)) {\n    # Extract current settings\n    mod_type &lt;- candidates$Model[i]\n    dist_type &lt;- candidates$Dist[i]\n    k_regimes &lt;- candidates$K[i]\n\n    # Create Spec\n    # Try creating spec based on regime count\n    # For MSGARCH, the number of regimes is inferred from the length of model/distribution vectors\n    spec &lt;- CreateSpec(\n      variance.spec = list(model = rep(mod_type, k_regimes)),\n      distribution.spec = list(distribution = rep(dist_type, k_regimes))\n    )\n\n    # Fit Model (Using ML for speed)\n    # We use tryCatch because complex models sometimes fail to converge\n    tryCatch({\n      fit &lt;- FitML(spec = spec, data = data_vector)\n\n      # Store Results\n      results[[i]] &lt;- data.frame(\n        Regimes = k_regimes,\n        Vol_Model = mod_type,\n        Distribution = dist_type,\n        AIC = AIC(fit),\n        BIC = BIC(fit)\n      )\n    }, error = function(e) {\n      # If fit fails, return NA\n      results[[i]] &lt;- data.frame(\n        Regimes = k_regimes,\n        Vol_Model = mod_type,\n        Distribution = dist_type,\n        AIC = NA,\n        BIC = NA\n      )\n    })\n  }\n\n  # Combine all results into one table\n  final_table &lt;- do.call(rbind, results)\n  return(final_table)\n}\n\n# 3. Run the Bake-Off (Example on ASX data)\n# Ensure you use the 'Return' vector you created in EDA\nbakeoff_results &lt;- run_bakeoff(asx_df$Return, candidates)\n\n# 4. Save results to CSV\nwrite.csv(bakeoff_results, \"bakeoff_results.csv\", row.names = FALSE)\n\n\n\n\nModel Bake-off Results\n# Read pre-computed results\nbakeoff_results &lt;- read.csv(\"bakeoff_results.csv\")\n\n# 4. Sort and Display Winner\nbakeoff_results &lt;- bakeoff_results %&gt;%\n  arrange(AIC) %&gt;% # Lowest AIC is best\n  mutate(\n    AIC_Rank = rank(AIC, na.last = \"keep\"), # Lower AIC = better rank\n    BIC_Rank = rank(BIC, na.last = \"keep\")  # Lower BIC = better rank\n  )\n\nkable(bakeoff_results, caption = \"Model Comparison: Sorted by AIC (Best on Top)\")\n\n\n\nModel Comparison: Sorted by AIC (Best on Top)\n\n\n\n\n\n\n\n\n\n\n\nRegimes\nVol_Model\nDistribution\nAIC\nBIC\nAIC_Rank\nBIC_Rank\n\n\n\n\n2\ngjrGARCH\nsstd\n15509.79\n15604.69\n1\n2\n\n\n1\ngjrGARCH\nsstd\n15538.58\n15579.25\n2\n1\n\n\n2\ngjrGARCH\nnorm\n15637.53\n15705.31\n3\n3\n\n\n2\nsGARCH\nsstd\n15658.98\n15740.32\n4\n5\n\n\n1\nsGARCH\nsstd\n15678.60\n15712.49\n5\n4\n\n\n1\ngjrGARCH\nnorm\n15767.84\n15794.95\n6\n6\n\n\n2\nsGARCH\nnorm\n15795.19\n15849.41\n7\n7\n\n\n1\nsGARCH\nnorm\n15949.59\n15969.92\n8\n8\n\n\n\n\n\nOkay, now we run into a little problem, the best model for AIC has BIC that is ranked 2nd, so which one should we choose?\n\nThe 2-Regime gjrGARCH with Skewed Student-t is the clear winner according to the Akaike Information Criterion (AIC), which prioritizes predictive accuracy.\nInterestingly, the 1-Regime version of the same model wins on BIC. This happens because BIC penalizes complexity (adding a second regime) more heavily.\nSince our goal is to capture market dynamics and regime shifts (which we know exist theoretically), the AIC winner (Rank 1) is usually the preferred choice as it captures the structural breaks that the 1-regime model misses.\n\n\n\n\n\n\n\nModel Selection Note\n\n\n\nIdeally, I would perform an expanding window and calculate mse to choose the best model here. However, it is going to take a lot of time so we are going to use AIC instead.\n\n\n\n\nFit the Model\n\nBest Model’s Specification:\nMathematically, the GJR-GARCH model (Glosten, Jagannathan, and Runkle, 1993) extends the standard GARCH framework by introducing an indicator term to capture the leverage effect—the empirical observation that negative returns (“bad news”) increase volatility more than positive returns (“good news”).\n1. The Variance Equation\nThe conditional variance \\(\\sigma_t^2\\) at time \\(t\\) is given by:\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\gamma \\epsilon_{t-1}^2 I_{t-1} + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(\\sigma_t^2\\): Forecasted variance for the current period.\n\\(\\omega &gt; 0\\): Baseline constant variance.\n\\(\\epsilon_{t-1}^2\\): The squared residual from the previous period (the “shock”).\n\\(\\beta \\ge 0\\): Persistence parameter (memory of past variance).\n\\(\\gamma\\) (Gamma): The asymmetry parameter that captures the leverage effect.\n\\(I_{t-1}\\) (Indicator Function): A binary switch defined as: \\[\n  I_{t-1} = \\begin{cases}\n  1 & \\text{if } \\epsilon_{t-1} &lt; 0 \\quad (\\text{Negative Shock / Bad News}) \\\\\n  0 & \\text{if } \\epsilon_{t-1} \\ge 0 \\quad (\\text{Positive Shock / Good News})\n  \\end{cases}\n  \\]\n\n2. Mechanism of Asymmetry\nThe total reaction to yesterday’s news depends on the sign of the shock:\n\nGood News (\\(\\epsilon_{t-1} \\ge 0\\)): The indicator \\(I_{t-1} = 0\\). The impact on volatility is \\(\\alpha \\epsilon_{t-1}^2\\).\nBad News (\\(\\epsilon_{t-1} &lt; 0\\)): The indicator \\(I_{t-1} = 1\\). The impact on volatility is \\((\\alpha + \\gamma) \\epsilon_{t-1}^2\\).\n\nSince \\(\\gamma\\) is typically positive, negative shocks generate a larger spike in volatility than positive shocks of the same magnitude.\n3. In the MS-GARCH Context\nIn a Markov-Switching framework with \\(K=2\\) regimes, the model estimates two distinct sets of these parameters. The variance at time \\(t\\) depends on the active regime \\(s_t\\):\n\\[\n\\sigma_t^2 = \\omega_{s_t} + (\\alpha_{s_t} + \\gamma_{s_t} I_{t-1}) \\epsilon_{t-1}^2 + \\beta_{s_t} \\sigma_{t-1}^2\n\\]\nThis allows the model to capture not just the leverage effect, but different intensities of the leverage effect during Calm (\\(s_t=1\\)) vs. Crisis (\\(s_t=2\\)) periods.\n\n\nFitting the Best MS-GARCH Model (Model Pre-fitted)\n# 1. Define the Best Specification (e.g., K=2, gjrGARCH, sstd)\n# MSGARCH infers the number of regimes from vector lengths\nspec_best &lt;- CreateSpec(\n  variance.spec = list(model = c(\"gjrGARCH\", \"gjrGARCH\")),\n  distribution.spec = list(distribution = c(\"sstd\", \"sstd\"))\n)\n\n# 2. Fit using Maximum Likelihood (MLE)\n# 'asx_df$Return' is your data vector from the EDA step\nfit_best &lt;- FitML(spec = spec_best, data = asx_df$Return)\n\n# 3. Save the fitted model\nsaveRDS(fit_best, \"fit_best_model.rds\")\n\n\n\n\nFitting the Best MS-GARCH Model\n# Load pre-computed model info\nmodel_info &lt;- readRDS(\"model_info.rds\")\n\n# 3. Display model information\ncat(\"Model fitted successfully!\\n\")\n\n\nModel fitted successfully!\n\n\nFitting the Best MS-GARCH Model\ncat(sprintf(\"AIC: %.2f\\n\", model_info$AIC))\n\n\nAIC: 15509.79\n\n\nFitting the Best MS-GARCH Model\ncat(sprintf(\"BIC: %.2f\\n\", model_info$BIC))\n\n\nBIC: 15604.69\n\n\nFitting the Best MS-GARCH Model\ncat(sprintf(\"Convergence: %d\\n\", model_info$convergence))\n\n\nI don’t know about you guys but this summary right here is what I can’t stand. I promise you, you nicer tables will be presented later but for now, I will show you some visualizations first before diving into what each of these numbers mean.\n\n\n\nInterpretation\n\n\nInteractive Plot with Fixed Segments\n# Load pre-computed model results\nmodel_results &lt;- read.csv(\"model_results.csv\")\n\n# ---------------------------------------------------------\n# 1. PREPARE THE DATA\n# ---------------------------------------------------------\n\n# Create a merged dataframe\ndf_plot &lt;- model_results %&gt;%\n  mutate(\n    # Create \"next day\" values for geom_segment\n    Date_Next = lead(Date),\n    Return_Next = lead(Return)\n  ) %&gt;%\n  # Remove the last row (NA due to lead)\n  na.omit()\n\n# Create separate datasets (same as before)\ndf_low &lt;- df_plot %&gt;% filter(Prob_HighVol &lt;= 0.33)\ndf_medium &lt;- df_plot %&gt;% filter(Prob_HighVol &gt; 0.33 & Prob_HighVol &lt;= 0.66)\ndf_high &lt;- df_plot %&gt;% filter(Prob_HighVol &gt; 0.66)\n\nplot_ly() %&gt;%\n  # 1. Low Probability (Green)\n  add_segments(data = df_low,\n               x = ~Date, y = ~Return,\n               xend = ~Date_Next, yend = ~Return_Next,\n               line = list(color = \"#28A745\", width = 2),\n               name = \"Regime 1 (≤33%)\",\n               hoverinfo = \"skip\") %&gt;%\n\n  # 2. Medium Probability (Orange)\n  add_segments(data = df_medium,\n               x = ~Date, y = ~Return,\n               xend = ~Date_Next, yend = ~Return_Next,\n               line = list(color = \"#EF7722\", width = 2),\n               name = \"Regime 1 (33-66%)\",\n               hoverinfo = \"skip\") %&gt;%\n\n  # 3. High Probability (Red)\n  add_segments(data = df_high,\n               x = ~Date, y = ~Return,\n               xend = ~Date_Next, yend = ~Return_Next,\n               line = list(color = \"#E62727\", width = 2),\n               name = \"Regime 2 (&gt;66%)\",\n               hoverinfo = \"skip\") %&gt;%\n  \n  # 4. Markers (Overlay points for hover info)\n  add_markers(data = df_plot,\n    x = ~Date,\n    y = ~Return,\n    marker = list(\n      color = \"black\",\n      size = 1,\n      opacity = 0.7\n    ),\n    hovertemplate = paste(\n      \"&lt;b&gt;Date:&lt;/b&gt; %{x}&lt;br&gt;\",\n      \"&lt;b&gt;Return:&lt;/b&gt; %{y:.4f}&lt;br&gt;\",\n      \"&lt;b&gt;Regime 2 Probability:&lt;/b&gt; %{customdata:.3f}&lt;br&gt;\",\n      \"&lt;extra&gt;&lt;/extra&gt;\"\n    ),\n    customdata = ~Prob_HighVol,\n    name = \"Daily Returns\"\n  ) %&gt;%\n  \n  # 5. Restore original layout and labels\n  colorbar(\n    title = list(text = \"Prob. Regime 2\", side = \"right\"),\n    len = 0.8\n  ) %&gt;%\n  layout(\n    title = list(\n      text = \"Asset Returns Colored by Regime Probability&lt;br&gt;&lt;sub&gt;Green = Regime 1 | Red = Regime 2&lt;/sub&gt;\",\n      font = list(size = 16)\n    ),\n    xaxis = list(\n      title = \"Date\",\n      type = \"date\"\n    ),\n    yaxis = list(\n      title = \"Log Returns (%)\"\n    ),\n    hovermode = \"closest\",\n    showlegend = FALSE\n  )\n\n\n\n\n\n\nWhile our model successfully distinguishes two volatility regimes, the interpretation is not so straightforward. It is not merely a distinction between ‘Low’ and ‘High’ volatility levels. Notably, the ‘Green’ regime encompasses the violent 2020 COVID-19 crash—an observation that would be contradictory if the regimes were classified solely by magnitude.\nInstead, the model has identified two structurally different types of volatility:\n\nRegime 1 (Green): Characterized by lower persistence but ‘fatter tails.’ This regime governs both normal market conditions and sudden, short-lived ‘flash crashes’ (like COVID-19) where the market recovers quickly.\nRegime 2 (Red): Characterized by extremely high persistence. This regime governs structural bear markets and systemic financial crises (like the GFC) where high volatility becomes entrenched for extended periods.\n\nTo understand the drivers behind this sophisticated distinction, we will now analyze the estimated parameters.\n\n\nCode\n# ---------------------------------------------------------\n# PRICE TIME SERIES COLORED BY REGIME PROBABILITY\n# ---------------------------------------------------------\n\n# Create a merged dataframe for prices using pre-computed results\ndf_price_plot &lt;- data.frame(\n  Date = asx_df$Date,\n  Price = asx_df$ASX200.Adjusted,\n  Prob_Regime2 = model_results$Prob_HighVol\n) %&gt;%\n  mutate(\n    # Create \"next day\" values for geom_segment\n    Date_Next = lead(Date),\n    Price_Next = lead(Price)\n  ) %&gt;%\n  # Remove the last row (NA due to lead)\n  na.omit()\n\n# Create separate datasets for each color category\ndf_price_low &lt;- df_price_plot %&gt;% filter(Prob_Regime2 &lt;= 0.33)\ndf_price_medium &lt;- df_price_plot %&gt;% filter(Prob_Regime2 &gt; 0.33 & Prob_Regime2 &lt;= 0.66)\ndf_price_high &lt;- df_price_plot %&gt;% filter(Prob_Regime2 &gt; 0.66)\n\n# Create interactive plot with separate colored traces for prices\nplot_ly() %&gt;%\n  # 1. Low Probability (Green)\n  add_segments(data = df_price_low,\n               x = ~Date, y = ~Price,\n               xend = ~Date_Next, yend = ~Price_Next,\n               line = list(color = \"#28A745\", width = 2),\n               name = \"Regime 1 (≤33%)\",\n               hoverinfo = \"skip\") %&gt;%\n\n  # 2. Medium Probability (Orange)\n  add_segments(data = df_price_medium,\n               x = ~Date, y = ~Price,\n               xend = ~Date_Next, yend = ~Price_Next,\n               line = list(color = \"#EF7722\", width = 2),\n               name = \"Regime 1 (33-66%)\",\n               hoverinfo = \"skip\") %&gt;%\n\n  # 3. High Probability (Red)\n  add_segments(data = df_price_high,\n               x = ~Date, y = ~Price,\n               xend = ~Date_Next, yend = ~Price_Next,\n               line = list(color = \"#E62727\", width = 2),\n               name = \"Regime 2 (&gt;66%)\",\n               hoverinfo = \"skip\") %&gt;%\n\n  # 4. Markers (Overlay points for hover info)\n  add_markers(data = df_price_plot,\n    x = ~Date,\n    y = ~Price,\n    marker = list(\n      color = \"black\",\n      size = 1,\n      opacity = 0.7\n    ),\n    hovertemplate = paste(\n      \"&lt;b&gt;Date:&lt;/b&gt; %{x}&lt;br&gt;\",\n      \"&lt;b&gt;Price:&lt;/b&gt; %{y:.2f}&lt;br&gt;\",\n      \"&lt;b&gt;Regime 2 Probability:&lt;/b&gt; %{customdata:.3f}&lt;br&gt;\",\n      \"&lt;extra&gt;&lt;/extra&gt;\"\n    ),\n    customdata = ~Prob_Regime2,\n    name = \"Daily Prices\"\n  ) %&gt;%\n\n  # 5. Restore original layout and labels\n  colorbar(\n    title = list(text = \"Prob. Regime 2\", side = \"right\"),\n    len = 0.8\n  ) %&gt;%\n  layout(\n    title = list(\n      text = \"ASX 200 Prices Colored by Regime Probability&lt;br&gt;&lt;sub&gt;Green = Regime 1 | Red = Regime 2&lt;/sub&gt;\",\n      font = list(size = 16)\n    ),\n    xaxis = list(\n      title = \"Date\",\n      type = \"date\"\n    ),\n    yaxis = list(\n      title = \"ASX 200 Price\"\n    ),\n    hovermode = \"closest\",\n    showlegend = FALSE\n  )\n\n\n\n\n\n\nThe time-series visualization highlights a profound distinction in market dynamics between the 2008 Global Financial Crisis and the 2020 COVID-19 crash.\nThe 2008 GFC is dominated by Regime 2 (Red), which is characterized by high persistence (\\(\\beta \\approx 0.93\\)). This aligns with the nature of a systemic banking crisis, where volatility ‘clustered’ for over 18 months as the structural integrity of the financial system was in question. The model correctly identifies this as a period where risk was entrenched and slow to decay.\nIn contrast, the 2020 COVID crash falls into Regime 1 (Green). Despite featuring the sharpest single-day declines in the dataset, the model identifies this as a ‘Fat Tail’ event (\\(\\nu \\approx 8.4\\)) rather than a ‘High Persistence’ event. This statistical classification perfectly mirrors the fundamental reality: the crash was an exogenous shock driven by news (the pandemic) rather than a structural financial failure. Consequently, the market recovery was ‘V-shaped’ and rapid, matching the lower persistence profile (\\(\\beta \\approx 0.84\\)) of Regime 1.\n\n\nCode\n# Load pre-computed coefficients\ncoef_df &lt;- read.csv(\"coef_df.csv\") %&gt;%\n  as_tibble()\n\n# Rename columns to avoid special characters\nnames(coef_df) &lt;- c(\"ParamLabel\", \"Estimate\", \"Std_Error\", \"t_value\", \"p_value\")\n\nprocess_params &lt;- function(df) {\n  df %&gt;%\n    filter(!str_detect(ParamLabel, \"^P_\")) %&gt;%\n    mutate(\n      Regime = str_extract(ParamLabel, \"\\\\d+$\"),\n      BaseParam = str_remove(ParamLabel, \"_\\\\d+$\"),\n      Symbol = case_when(\n        BaseParam == \"alpha0\" ~ \"$\\\\omega$ (Constant)\",\n        BaseParam == \"alpha1\" ~ \"$\\\\alpha$ (ARCH)\",\n        BaseParam == \"alpha2\" ~ \"$\\\\gamma$ (Leverage)\",\n        BaseParam == \"beta\"   ~ \"$\\\\beta$ (Persistence)\",\n        BaseParam == \"nu\"     ~ \"$\\\\nu$ (Degree of Freedom)\",\n        BaseParam == \"xi\"     ~ \"$\\\\xi$ (Skewness)\",\n        TRUE ~ BaseParam\n      ),\n      SortOrder = case_when(\n        BaseParam == \"alpha0\" ~ 1,\n        BaseParam == \"alpha1\" ~ 2,\n        BaseParam == \"alpha2\" ~ 3,\n        BaseParam == \"beta\"   ~ 4,\n        BaseParam == \"nu\"     ~ 5,\n        BaseParam == \"xi\"     ~ 6,\n        TRUE ~ 99\n      ),\n      FormattedVal = paste0(\n        sprintf(\"%.4f\", Estimate),\n        \" (p \",\n        ifelse(p_value &lt; 0.001, \"&lt; 0.001\", sprintf(\"= %.3f\", p_value)),\n        \")\"\n      )\n    ) %&gt;%\n    select(Symbol, SortOrder, Regime, FormattedVal) %&gt;%\n    pivot_wider(names_from = Regime, values_from = FormattedVal, names_prefix = \"Regime \") %&gt;%\n    arrange(SortOrder) %&gt;%\n    select(-SortOrder)\n}\n\ntable1_data &lt;- process_params(coef_df)\n\nkable(table1_data, caption = \"MS-GJR-GARCH Parameter Estimates\", align = 'c')\n\n\n\nMS-GJR-GARCH Parameter Estimates\n\n\nSymbol\nRegime 1\nRegime 2\n\n\n\n\n\\(\\omega\\) (Constant)\n0.0452 (p &lt; 0.001)\n0.0066 (p &lt; 0.001)\n\n\n\\(\\alpha\\) (ARCH)\n0.0000 (p = 0.486)\n0.0001 (p = 0.493)\n\n\n\\(\\gamma\\) (Leverage)\n0.1662 (p &lt; 0.001)\n0.1200 (p &lt; 0.001)\n\n\n\\(\\beta\\) (Persistence)\n0.8458 (p &lt; 0.001)\n0.9322 (p &lt; 0.001)\n\n\n\\(\\nu\\) (Degree of Freedom)\n8.4213 (p &lt; 0.001)\n19.7948 (p = 0.002)\n\n\n\\(\\xi\\) (Skewness)\n0.7782 (p &lt; 0.001)\n0.8892 (p &lt; 0.001)\n\n\n\n\n\nInterpretation of Parameter Estimates\nThe estimated parameters from the MS-GJR-GARCH model provide the mathematical “fingerprint” for each regime, explaining the distinct market behaviors observed in the time-series plots.\n1. Persistence (\\(\\beta\\)): The “Memory” of the Market\n\nRegime 1 (\\(\\beta \\approx 0.85\\)): This lower persistence value indicates that volatility shocks in this regime decay relatively quickly. This explains the “V-shaped” recovery of the 2020 COVID-19 crash; the market panic was intense but did not entrench itself into a long-term trend.\nRegime 2 (\\(\\beta \\approx 0.93\\)): The significantly higher beta value indicates “long memory.” In this regime, high volatility today is a strong predictor of high volatility tomorrow. This mathematical structure creates the “sticky,” clustered volatility observed during the 18-month duration of the Global Financial Crisis (2008).\n\n2. Degrees of Freedom (\\(\\nu\\)): The “Black Swan” Indicator\n\nRegime 1 (\\(\\nu \\approx 8.42\\)): A low Degree of Freedom indicates a “fat-tailed” distribution where extreme outliers are more probable. This tells the model to expect sudden, violent shocks (like the flash crash of March 2020) as a feature of this regime, rather than a structural break.\nRegime 2 (\\(\\nu \\approx 19.79\\)): A higher Degree of Freedom implies a distribution closer to Normality. This suggests that the risk in the “Red” regime is not driven by random outliers, but rather by the sustained, high-variance trend itself.\n\n3. The Leverage Effect (\\(\\gamma\\) vs \\(\\alpha\\)): Sensitivity to Bad News\n\nAlpha (\\(\\alpha \\approx 0\\)): In both regimes, the symmetric ARCH term is effectively zero, meaning positive returns (market rallies) have negligible impact on future volatility.\nGamma (\\(\\gamma &gt; 0\\)): Both regimes show a significant positive Gamma, confirming the “Leverage Effect”—volatility is driven almost entirely by negative returns (panic selling).\nComparative Insight: Interestingly, Regime 1 has a higher Gamma (\\(0.166\\)) than Regime 2 (\\(0.120\\)). This implies that the market is more sensitive to bad news during “normal” or “flash” periods than during deep structural crises. In a crisis (Regime 2), high volatility is already the baseline, so new negative information has a marginally smaller shock effect than it does in a calmer environment.\n\n4. Skewness (\\(\\xi\\)): The Downside Bias\n\nThe skewness parameter \\(\\xi\\) is below 1.0 for both regimes, indicating a negative skew (left tail is longer). However, Regime 1 is more negatively skewed (\\(0.78\\)) than Regime 2 (\\(0.89\\)). This reinforces the characterization of Regime 1 as the home of “Flash Crashes”—structurally more prone to sudden, sharp downside moves than the grinding volatility of Regime 2.\n\n\n\nCode\n# Load pre-computed transition data\ntransition_data &lt;- read.csv(\"transition_data.csv\")\n\nkable(transition_data, digits = 4, caption = \"Regime Probabilities & Persistence\", align = 'c')\n\n\n\nRegime Probabilities & Persistence\n\n\nMetric\nRegime_1\nRegime_2\n\n\n\n\nTransition Probability (P_ii)\n0.9964\n0.9965\n\n\nExpected Duration (Days)\n276.2344\n288.3764\n\n\nUnconditional Probability\n0.4892\n0.5108\n\n\n\n\n\nThe “Regime Probabilities & Persistence” table characterizes the stability and frequency of the two volatility regimes detected by the model. These metrics indicate that the market transitions between states gradually rather than abruptly.\n1. Transition Probability (\\(P_{ii}\\)): Regime Stability\n\nRegime 1 (\\(P_{11} \\approx 0.9964\\)): The probability of remaining in Regime 1 the following day, given the market is currently in Regime 1, is 99.64%.\nRegime 2 (\\(P_{22} \\approx 0.9965\\)): Similarly, the probability of remaining in Regime 2 the following day is 99.65%.\nImplication: The high values for both diagonal elements of the transition matrix (\\(P_{11}\\) and \\(P_{22}\\)) indicate a high degree of persistence. Once a volatility regime is established, it is statistically unlikely to switch states on any given day. This suggests that volatility clusters into stable blocks rather than fluctuating randomly.\n\n2. Expected Duration: Average Regime Length\n\nRegime 1 Duration: \\(\\approx 276\\) trading days.\nRegime 2 Duration: \\(\\approx 288\\) trading days.\nImplication: Based on the transition probabilities, the expected duration for each regime exceeds one standard trading year (typically ~252 days). This result characterizes the identified regimes as medium-to-long-term market phases (economic climates) rather than short-term transient events.\n\n3. Unconditional Probability: Long-Term Distribution\n\nRegime 1: \\(48.9\\%\\)\nRegime 2: \\(51.1\\%\\)\nImplication: The unconditional probability represents the likelihood of the market being in a specific regime at any random point in time over the long run. The near-even split indicates that the “High Persistence” state (Regime 2) is a recurring structural feature of the ASX 200, occurring with roughly the same frequency as the “Low Persistence” state (Regime 1).\n\n\n\nModel Diagnostics\nTo validate the reliability of the MS-GJR-GARCH model, we perform diagnostic checks on the residuals to ensure no statistical patterns remain.\n\n\nCode\n# 1. Force reload data using Base R (most robust method for this specific chunk)\nraw_data &lt;- read.csv(\"ASX200_Fixed.csv\")\n\n# 2. Simple return calculation\nprices &lt;- raw_data$ASX200.Adjusted\n# Calculate log returns: diff(log(price)) * 100\nret_vec &lt;- diff(log(prices)) * 100\n# Remove the first NA created by diff\nret_vec &lt;- na.omit(ret_vec)\n# Ensure it is a pure numeric vector (strip attributes)\nret_vec &lt;- as.vector(ret_vec)\n\n# 3. Create Spec and Fit Model directly\nlibrary(MSGARCH) # Ensure library is loaded\nspec_simple &lt;- CreateSpec(\n  variance.spec = list(model = c(\"gjrGARCH\", \"gjrGARCH\")),\n  distribution.spec = list(distribution = c(\"sstd\", \"sstd\"))\n)\n\n# Fit the model\nfit_object &lt;- FitML(spec = spec_simple, data = ret_vec)\n\n# 4. Generate Diagnostics\n# Explicitly pass 'x = ret_vec' to avoid \"data is NULL\" errors\nvol_dynamic &lt;- Volatility(fit_object, x = ret_vec)\npit_values &lt;- PIT(fit_object, x = ret_vec)\nstd_resid &lt;- ret_vec / vol_dynamic\n\n# 5. Plotting\npar(mfrow = c(1, 2))\n\n# ACF Plot\nacf(std_resid^2,\n    main = \"ACF of Squared Std. Residuals\",\n    lag.max = 20,\n    ylim = c(-0.1, 0.5))\n\n# PIT Histogram\nhist(pnorm(pit_values),\n     breaks = 20,\n     main = \"PIT Histogram (Uniform Transform)\",\n     xlab = \"Probability\",\n     col = \"gray\",\n     border = \"white\",\n     freq = FALSE)\nabline(h = 1, col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Diagnostics generated successfully.\\n\")\n\n\nDiagnostics generated successfully.\n\n\n1. ACF of Squared Standardized Residuals\n\nWhy do we need it?: We need the ACF plot to verify that our model successfully extracted all the “volatility clustering” (predictable time patterns) from the data. This ensures the remaining noise is truly random and has no “memory” left over.\nWhat do we want?: The bars represent the autocorrelation between volatility today and volatility \\(k\\) days ago. The red dotted lines are the “significance bands” (95% confidence interval). Ideally, we want all bars to stay inside these bands, meaning any remaining correlation is effectively zero.\nInterpretation: Our ACF plot shows that almost all bars are well within the 95% confidence intervals, indicating no significant leftover patterns. This confirms the model has adequately accounted for the conditional heteroskedasticity, validating that the volatility clustering has been fully captured.\n\n2. PIT (Probability Integral Transform) Histogram\n\nWhy do we need it?: We need the PIT histogram to verify that our chosen mathematical distribution (the Skewed Student-t) matches the actual shape of the market data. If we chose the wrong distribution (e.g., Normal), our risk estimates (like Value-at-Risk) would be dangerously inaccurate.\nWhat do we want?: We want the histogram to look flat and uniform, like a rectangle. A U-shape implies the model underestimated extreme risks (“fat tails”), while a Hill-shape implies it overestimated them.\nInterpretation: Our PIT histogram is relatively flat and uniform, without any distinct U-shape. This indicates that the Skewed Student-t distribution was the correct choice. It successfully captures the asymmetry and “fat tails” of the ASX 200, whereas a standard Normal distribution would likely have failed this test.\n\n\n\nForecasting and Validation\nHaving estimated the model parameters, the critical question remains: Does this added complexity actually translate to better predictive power? To answer this, we will validate the model in two stages: an in-sample visual inspection and a rigorous out-of-sample forecasting simulation.\n\n1. In-Sample Visual Inspection\nFirst, we perform a “sanity check.” We overlay the model’s estimated volatility (\\(\\sigma_t\\)) onto the historical returns. While we cannot observe true volatility, we can compare the model’s output against the magnitude of daily returns (\\(|r_t|\\)).\nWhat we are looking for: We want to ensure the model’s volatility spikes align precisely with periods of market turbulence (high absolute returns) and that it calms down during stable periods.\n\n\nInteractive Plot MS-GJR-GARCH In-Sample Volatility vs. Realized Shocks\n# 1. Ensure Data Availability (Self-healing)\nif (!exists(\"asx_df\") || is.null(asx_df$Return)) {\n  asx_df &lt;- read.csv(\"ASX200_Fixed.csv\")\n  asx_df$Date &lt;- as.Date(asx_df$Date)\n  asx_df$Return &lt;- c(NA, diff(log(asx_df$ASX200.Adjusted))) * 100\n  asx_df &lt;- na.omit(asx_df)\n}\n\n# 2. Get True Model Volatility\n# We refit quickly to get the exact path (ensures consistency with diagnostics)\nlibrary(MSGARCH)\nspec_best &lt;- CreateSpec(\n  variance.spec = list(model = c(\"gjrGARCH\", \"gjrGARCH\")),\n  distribution.spec = list(distribution = c(\"sstd\", \"sstd\"))\n)\nfit_best &lt;- FitML(spec = spec_best, data = asx_df$Return)\nvol_model &lt;- Volatility(fit_best, x = asx_df$Return)\n\n# 3. Create Plotting Dataframe\ncomparison_df &lt;- data.frame(\n  Date = asx_df$Date,\n  Actual_Magnitude = abs(asx_df$Return), # |r_t| proxy for realized vol\n  Predicted_Vol = vol_model              # Actual MS-GARCH sigma_t\n)\n\n# 4. Create Interactive Plot\nplot_ly(comparison_df, x = ~Date) %&gt;%\n  # Layer 1: The \"Actual\" Market Shocks (Gray)\n  add_lines(y = ~Actual_Magnitude,\n            name = \"Actual Magnitude (|r|)\",\n            line = list(color = \"gray70\", width = 1),\n            opacity = 0.4,\n            hovertemplate = paste(\"Date: %{x|%Y-%m-%d}&lt;br&gt;\",\n                                 \"Actual Magnitude: %{y:.3f}%&lt;br&gt;\",\n                                 \"&lt;extra&gt;&lt;/extra&gt;\")) %&gt;%\n\n  # Layer 2: The Model's Prediction (Red)\n  add_lines(y = ~Predicted_Vol,\n            name = \"MS-GJR-GARCH Volatility (σ)\",\n            line = list(color = \"#D9534F\", width = 1.5),\n            hovertemplate = paste(\"Date: %{x|%Y-%m-%d}&lt;br&gt;\",\n                                 \"Model Volatility: %{y:.3f}%&lt;br&gt;\",\n                                 \"&lt;extra&gt;&lt;/extra&gt;\")) %&gt;%\n\n  # Layout and styling\n  layout(\n    title = list(\n      text = \"In-Sample Fit: Model-Implied Volatility vs. Realized Shocks\",\n      subtitle = \"Red line = The volatility regime estimated by the model\"\n    ),\n    xaxis = list(\n      title = \"Date\",\n      type = \"date\"\n    ),\n    yaxis = list(\n      title = \"Volatility (%)\"\n    ),\n    legend = list(\n      orientation = \"h\",\n      x = 0.5,\n      xanchor = \"center\",\n      y = -0.2\n    ),\n    hovermode = \"x unified\"\n  )\n\n\n\n\n\n\n\n\n2. Out-of-Sample Testing: The Expanding Window Approach\nVisual fit is often misleading because the model has already “seen” the data. To test how the model would perform in a real trading environment, we employ an Expanding Window Cross-Validation.\nThe Simulation Strategy: Instead of a simple train/test split, we simulate a trader re-calibrating their model every week.\n\nStart: We train the model on the initial 500 days.\nForecast: We predict volatility for the next 5 days (one trading week).\nExpand: We add those 5 days to our training set.\nRefit & Repeat: We re-estimate all parameters and forecast the next week, repeating this process until we reach the end of the dataset.\n\nThis method is computationally expensive but eliminates “look-ahead bias” and proves the model can adapt to new regimes in real-time.\n\n\nFast Parallel Cross-Validation (Results Pre-computed)\n# 1. Full Dataset CV Setup (Rolling Window)\nn_total &lt;- length(asx_df$Return)\nmin_train_size &lt;- 500  # Minimum training window\nrefit_every &lt;- 5       # Refit every 5 days\n\n# Use all observations for CV - rolling window approach\nn_test &lt;- n_total - min_train_size\n\n# 2. Setup\n\n# Detect your cores (Leave 1 free for OS)\nn_cores &lt;- parallel::detectCores() - 1\nregisterDoParallel(cores = n_cores)\ncat(sprintf(\"Running in parallel on %d cores...\\n\", n_cores))\n\n# Define refit indices for full dataset CV\nrefit_indices &lt;- seq(from = 1, to = n_test, by = refit_every)\n\n# Parallel CV on full dataset\nresults_list &lt;- foreach(i = refit_indices, .packages = c('MSGARCH')) %dopar% {\n  # Current position in full dataset\n  current_pos &lt;- min_train_size + i - 1\n\n  # Training window: expanding from minimum size\n  train_data &lt;- asx_df$Return[1:current_pos]\n\n  # Create and fit model\n  spec_local &lt;- CreateSpec(\n    variance.spec = list(model = c(\"gjrGARCH\", \"gjrGARCH\")),\n    distribution.spec = list(distribution = c(\"sstd\", \"sstd\"))\n  )\n\n  fit &lt;- FitML(spec = spec_local, data = train_data)\n\n  # Predict next 5 days\n  pred &lt;- predict(fit, nahead = refit_every, do.return.draw = FALSE)\n\n  data.frame(\n    Step_Index = i:(min(i + refit_every - 1, n_test)),\n    Predicted_Vol = pred$vol[1:min(refit_every, n_test - i + 1)]\n  )\n}\n\n# 3. Combine Results\nfinal_results &lt;- do.call(rbind, results_list)\nstopImplicitCluster()\n\n# Clean up and merge with actual data for full dataset CV\nfinal_results &lt;- final_results[order(final_results$Step_Index), ]\nfinal_results &lt;- final_results[!duplicated(final_results$Step_Index), ]\n\ntest_indices &lt;- (min_train_size + 1):(min_train_size + nrow(final_results))\nfinal_results$Actual_Return &lt;- asx_df$Return[test_indices]\nfinal_results$Date &lt;- asx_df$Date[test_indices]\n\n# 4. Calculate CV Metrics for 2-Regime\ncv_metrics_2regime &lt;- data.frame(\n  MSE = mean((final_results$Predicted_Vol - abs(final_results$Actual_Return))^2),\n  RMSE = sqrt(mean((final_results$Predicted_Vol - abs(final_results$Actual_Return))^2)),\n  MAE = mean(abs(final_results$Predicted_Vol - abs(final_results$Actual_Return))),\n  Model = \"2-Regime MS-GJR-GARCH\"\n)\n\n# 5. Save results to CSV\nwrite.csv(final_results, \"cv_results_2regime.csv\", row.names = FALSE)\nwrite.csv(cv_metrics_2regime, \"cv_metrics_2regime.csv\", row.names = FALSE)\n\n\n\n\nFast Parallel Cross-Validation\n# Load pre-computed results\nfinal_results &lt;- read.csv(\"cv_results_2regime.csv\")\ncv_metrics_2regime &lt;- read.csv(\"cv_metrics_2regime.csv\")\n\n\n\n\n3. Benchmarking: Single-Regime Model Comparison\nIs the Regime-Switching component actually necessary? To prove the value of the Hidden Markov Model, we must compare it against a benchmark.\nWe will run the exact same expanding window simulation using a standard 1-Regime GJR-GARCH. If our complex 2-Regime model cannot beat this simpler benchmark, then the added complexity is not justified.\n\n\nSingle-Regime Cross-Validation (Parallel) (Results Pre-computed)\n# Setup parallel processing for 1-regime model\nn_cores &lt;- parallel::detectCores() - 1\nregisterDoParallel(cores = n_cores)\n\n# Define indices for parallel processing (refit every 5 days)\nrefit_indices &lt;- seq(from = 1, to = n_test, by = 5)\n\n# Parallel CV for 1-regime model on full dataset\nresults_1reg_list &lt;- foreach(i = refit_indices, .packages = c('MSGARCH')) %dopar% {\n  current_pos &lt;- min_train_size + i - 1\n  train_data &lt;- asx_df$Return[1:current_pos]\n\n  spec_1reg &lt;- CreateSpec(\n    variance.spec = list(model = \"gjrGARCH\"),\n    distribution.spec = list(distribution = \"sstd\")\n  )\n\n  fit &lt;- FitML(spec = spec_1reg, data = train_data)\n  pred &lt;- predict(fit, nahead = refit_every, do.return.draw = FALSE)\n\n  data.frame(\n    Step_Index = i:(min(i + refit_every - 1, n_test)),\n    Predicted_Vol = pred$vol[1:min(refit_every, n_test - i + 1)]\n  )\n}\n\n# Combine 1-regime results\nresults_1reg &lt;- do.call(rbind, results_1reg_list)\nstopImplicitCluster()\n\n# Clean up 1-regime results\nresults_1reg &lt;- results_1reg[order(results_1reg$Step_Index), ]\nresults_1reg &lt;- results_1reg[!duplicated(results_1reg$Step_Index), ]\n\ntest_indices_1reg &lt;- (min_train_size + 1):(min_train_size + nrow(results_1reg))\nresults_1reg$Actual_Return &lt;- asx_df$Return[test_indices_1reg]\nresults_1reg$Date &lt;- asx_df$Date[test_indices_1reg]\n\n# Calculate CV metrics for 1-regime\ncv_metrics_1regime &lt;- data.frame(\n  MSE = mean((results_1reg$Predicted_Vol - abs(results_1reg$Actual_Return))^2),\n  RMSE = sqrt(mean((results_1reg$Predicted_Vol - abs(results_1reg$Actual_Return))^2)),\n  MAE = mean(abs(results_1reg$Predicted_Vol - abs(results_1reg$Actual_Return))),\n  Model = \"1-Regime GJR-GARCH\"\n)\n\n# Save results to CSV\nwrite.csv(results_1reg, \"cv_results_1regime.csv\", row.names = FALSE)\nwrite.csv(cv_metrics_1regime, \"cv_metrics_1regime.csv\", row.names = FALSE)\n\n\n\n\nSingle-Regime Cross-Validation (Parallel)\n# Load pre-computed results\nresults_1reg &lt;- read.csv(\"cv_results_1regime.csv\")\ncv_metrics_1regime &lt;- read.csv(\"cv_metrics_1regime.csv\")\n\n\n\n\n4. The ‘Horse Race’: Visualizing Performance\nNow we compare the forecasts side-by-side. The plot below zooms in on the 2008 Global Financial Crisis. We are looking for the speed of reaction: Does the 2-Regime model (Red) react faster to the onset of the crisis than the standard model (Blue)?\n\n\nInteractive Model Comparison Plots\n# Combine results for comparison\nfinal_results$Model &lt;- \"2-Regime MS-GJR-GARCH\"\nresults_1reg$Model &lt;- \"1-Regime GJR-GARCH\"\n\n# Ensure Date columns are properly formatted as Date objects (not character strings)\nfinal_results$Date &lt;- as.Date(final_results$Date)\nresults_1reg$Date &lt;- as.Date(results_1reg$Date)\n\ncombined_results &lt;- rbind(\n  data.frame(\n    Date = final_results$Date,\n    Actual_Return = final_results$Actual_Return,\n    Actual_Mag = abs(final_results$Actual_Return),\n    Predicted_Vol = final_results$Predicted_Vol,\n    Model = final_results$Model\n  ),\n  data.frame(\n    Date = results_1reg$Date,\n    Actual_Return = results_1reg$Actual_Return,\n    Actual_Mag = abs(results_1reg$Actual_Return),\n    Predicted_Vol = results_1reg$Predicted_Vol,\n    Model = results_1reg$Model\n  )\n)\n\n# Interactive plot for 2008 crisis period\nplot_2008_data &lt;- combined_results %&gt;%\n  filter(Date &gt;= \"2007-01-01\" & Date &lt;= \"2009-12-31\")\n\nplot_2008 &lt;- plot_ly(plot_2008_data, x = ~Date) %&gt;%\n  add_lines(y = ~Actual_Mag, name = \"Realized Abs. Return (Proxy)\",\n            line = list(color = \"gray\", width = 2), opacity = 0.6) %&gt;%\n  add_lines(data = plot_2008_data %&gt;% filter(Model == \"2-Regime MS-GJR-GARCH\"),\n            y = ~Predicted_Vol, name = \"2-Regime Model\",\n            line = list(color = \"red\", width = 3)) %&gt;%\n  add_lines(data = plot_2008_data %&gt;% filter(Model == \"1-Regime GJR-GARCH\"),\n            y = ~Predicted_Vol, name = \"1-Regime Model\",\n            line = list(color = \"blue\", width = 3, dash = \"dash\")) %&gt;%\n  layout(\n    title = list(\n      text = \"Model Comparison: Out-of-Sample Performance during GFC (2008)\",\n      subtitle = \"Comparing Model Sigma against Realized Absolute Returns\"\n    ),\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Volatility (%)\"),\n    legend = list(x = 0.02, y = 0.98),\n    hovermode = \"x unified\"\n  )\n\nplot_2008\n\n\n\n\n\n\n\n\nInteractive Full Period Comparison\n# Interactive plot for full test period\nplot_full &lt;- plot_ly(combined_results, x = ~Date) %&gt;%\n  add_lines(y = ~Actual_Mag, name = \"Realized Abs. Return (Proxy)\",\n            line = list(color = \"gray\", width = 1), opacity = 0.5) %&gt;%\n  add_lines(data = combined_results %&gt;% filter(Model == \"2-Regime MS-GJR-GARCH\"),\n            y = ~Predicted_Vol, name = \"2-Regime Model\",\n            line = list(color = \"red\", width = 2)) %&gt;%\n  add_lines(data = combined_results %&gt;% filter(Model == \"1-Regime GJR-GARCH\"),\n            y = ~Predicted_Vol, name = \"1-Regime Model\",\n            line = list(color = \"blue\", width = 2, dash = \"dash\")) %&gt;%\n  layout(\n    title = list(\n      text = \"Full Period Model Comparison: Predicted vs Actual Volatility\",\n      subtitle = \"Comparing Model Sigma against Realized Absolute Returns\"\n    ),\n    xaxis = list(\n      title = \"Date\",\n      tickformat = \"%Y\",      # Keep axis clean (Years only)\n      hoverformat = \"%Y-%m-%d\", # Force hover to show full date (e.g. 2008-09-15)\n      dtick = \"M24\"\n    ),\n    yaxis = list(title = \"Volatility / Magnitude\"),\n    legend = list(x = 0.02, y = 0.98),\n    hovermode = \"x unified\"\n  )\n\nplot_full\n\n\n\n\n\n\n\n\n5. The Scorecard: Error Metrics\nFinally, we quantify the performance difference. Since ‘True Volatility’ is unobservable, we use Realized Absolute Returns (\\(|r_t|\\)) as our proxy for truth. We compare the models across three standard metrics:\n\nMSE (Mean Squared Error): Penalizes large errors heavily.\nRMSE (Root Mean Squared Error): Interpretable in the same units as volatility.\nMAE (Mean Absolute Error): Measures the average magnitude of the error.\n\n\n\nPrediction Accuracy Comparison Table\n# Combine metrics\ncombined_metrics &lt;- rbind(cv_metrics_2regime, cv_metrics_1regime)\n\n# Create comparison table\n\ncomparison_table &lt;- combined_metrics %&gt;%\n  select(Model, MSE, RMSE, MAE) %&gt;%\n  mutate(\n    MSE = round(MSE, 4),\n    RMSE = round(RMSE, 4),\n    MAE = round(MAE, 4)\n  )\n\nkable(comparison_table,\n      caption = \"Cross-Validation Prediction Accuracy Metrics\",\n      col.names = c(\"Model\", \"MSE\", \"RMSE\", \"MAE\"),\n      align = c('l', 'c', 'c', 'c')) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n                full_width = FALSE) %&gt;%\n  row_spec(which.min(comparison_table$MSE), background = \"#e6f3ff\") %&gt;%\n  row_spec(which.min(comparison_table$RMSE), background = \"#e6f3ff\") %&gt;%\n  row_spec(which.min(comparison_table$MAE), background = \"#e6f3ff\")\n\n\n\nCross-Validation Prediction Accuracy Metrics\n\n\nModel\nMSE\nRMSE\nMAE\n\n\n\n\n2-Regime MS-GJR-GARCH\n0.4601\n0.6783\n0.5026\n\n\n1-Regime GJR-GARCH\n0.4653\n0.6821\n0.5039\n\n\n\n\n\n\n\nThe out-of-sample performance metrics confirm that the 2-Regime MS-GJR-GARCH model outperforms the benchmark 1-Regime GJR-GARCH, achieving lower error rates across all evaluated categories (MSE, RMSE, and MAE).\nSpecifically, the regime-switching specification reduced the Mean Squared Error (MSE) by 1.35% (from 0.4659 to 0.4596) when benchmarked against realized absolute returns. While the numerical improvement is marginal, the consistency of the reduction across all metrics suggests that the inclusion of regime dynamics provides a tangible predictive edge, particularly in capturing the speed of mean reversion.\nFurthermore, the primary advantage of the MS-GJR-GARCH framework extends beyond raw error minimization: it offers the distinct capability to disentangle structural volatility (Regime 2) from transitory shocks (Regime 1), a feature that the single-regime benchmark statistically cannot provide."
  },
  {
    "objectID": "projects/project-1/index.html",
    "href": "projects/project-1/index.html",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "",
    "text": "In this project, my aim is to forecaste the Australian CPI values for 2024 Q1-Q4 using ARIMA models. Quarterly CPI data from March 1995 to December 2023 (114 observations) serves as training data, with March-December 2024 (4 observations) held out for testing."
  },
  {
    "objectID": "projects/project-1/index.html#overview",
    "href": "projects/project-1/index.html#overview",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "",
    "text": "In this project, my aim is to forecaste the Australian CPI values for 2024 Q1-Q4 using ARIMA models. Quarterly CPI data from March 1995 to December 2023 (114 observations) serves as training data, with March-December 2024 (4 observations) held out for testing."
  },
  {
    "objectID": "projects/project-1/index.html#data-loading-and-exploration",
    "href": "projects/project-1/index.html#data-loading-and-exploration",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "Data Loading and Exploration",
    "text": "Data Loading and Exploration\nFirst, we load the Australian CPI data.\n\n# Load the Australian CPI data\ndf = pd.read_csv('data.cpi.aus.csv')\nprint(\"First 5 observations:\")\nprint(df.head())\n\nFirst 5 observations:\n       Date  CPI\n0  Jun-1922  2.8\n1  Sep-1922  2.8\n2  Dec-1922  2.7\n3  Mar-1923  2.7\n4  Jun-1923  2.8\n\n\nSince the goal of this project is to forecast the Australian CPI, we need to split the data into train and test sets.\nThe training set is from March 1995 to December 2023, and the test set is from March 2024 to December 2024.\n\n# Convert Date column to datetime for proper filtering\ndf['Date'] = pd.to_datetime(df['Date'], format='%b-%Y')\n\n# Split data into train and test sets\n# Train: Mar-1995 to Dec-2023\n# Test: Mar-2024 to Dec-2024\ntrain_start = pd.to_datetime('1995-03-01')\ntrain_end = pd.to_datetime('2023-12-01')\ntest_start = pd.to_datetime('2024-03-01')\ntest_end = pd.to_datetime('2024-12-01')\n\ndf = df[(df['Date'] &gt;= train_start) & (df['Date'] &lt;= test_end)].copy()\n# Filter the data\ntrain_df = df[(df['Date'] &gt;= train_start) & (df['Date'] &lt;= train_end)].copy()\ntest_df = df[(df['Date'] &gt;= test_start) & (df['Date'] &lt;= test_end)].copy()\n\nLet’s plot the original CPI data.\n\n\nCode\n# Set Date as index and sort for time series analysis\ndf_ts = df.set_index('Date').sort_index()\n\n# Extract CPI series\ncpi_series = df_ts['CPI']\n\n# Plot original CPI data\nplt.figure(figsize=(8, 4))\n\n# Add light red shaded region for forecast period (Mar-2024 to Dec-2024)\nforecast_start = pd.to_datetime('2024-03-01')\nforecast_end = pd.to_datetime('2024-12-01')\nplt.axvspan(forecast_start, forecast_end, alpha=0.2, color='red', label='Forecast Period')\n\nplt.plot(cpi_series.index, cpi_series, color='black', label='CPI')\nplt.axvline(pd.to_datetime('2020-01-01'), color='red', linestyle='--', label='COVID-19 (2020)', alpha=0.8)\n\nplt.title('Australian Consumer Price Index (CPI)')\nplt.xlabel('Date')\nplt.ylabel('CPI Value')\nplt.legend()\nplt.grid(False)\nax_current = plt.gca()\nsns.despine(ax=ax_current, top=True, right=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Original CPI data\n\n\n\n\n\nThe plot shows the original CPI data. The light red shaded region indicates the forecast period (March 2024 to December 2024), and the red vertical line indicates the year 2020, which marks the COVID-19 pandemic.\n\nSummary of the training CPI data\n\n\nCode\n# Create training series (exclude test data)\ntrain_cpi = train_df.set_index('Date')['CPI']\n\n# Note: Frequency setting removed because dates don't conform to standard quarterly pattern\n# The data is quarterly (every 3 months) but dates are irregular quarter endings\n# Statsmodels will infer frequency as needed, though it may show warnings\n\n# Boxplot of training CPI values\nplt.figure(figsize=(8, 4))\nplt.boxplot(train_cpi.values, vert=False, patch_artist=True,\n            boxprops=dict(facecolor='lightblue', color='black'),\n            medianprops=dict(color='red', linewidth=2),\n            whiskerprops=dict(color='black'),\n            capprops=dict(color='black'))\n\nplt.title('Distribution of Australian CPI Values (Training Data: 1995-2023)')\nplt.xlabel('CPI Value')\nplt.yticks([])  # Remove y-axis ticks for a horizontal boxplot\n\n# Annotate with key statistics from training data\nstats = train_cpi.describe()\nplt.text(stats['min'], 1.1, f'Min: {stats[\"min\"]:.1f}', ha='center', va='bottom', fontsize=9)\nplt.text(stats['25%'], 1.1, f'Q1: {stats[\"25%\"]:.1f}', ha='center', va='bottom', fontsize=9)\nplt.text(stats['50%'], 1.1, f'Median: {stats[\"50%\"]:.1f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\nplt.text(stats['75%'], 1.1, f'Q3: {stats[\"75%\"]:.1f}', ha='center', va='bottom', fontsize=9)\nplt.text(stats['max'], 1.1, f'Max: {stats[\"max\"]:.1f}', ha='center', va='bottom', fontsize=9)\n\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nBoxplot of training CPI values\n\n\n\n\n\n\nSTL Decomposition of the training CPI data\n\n\nCode\n# Perform STL decomposition on training data only\nstl = STL(train_cpi, period=12)\nstl_result = stl.fit()\n\n# Plot STL decomposition\nplt.figure(figsize=(8, 6))\nplt.subplot(411)\nplt.plot(train_cpi, label='Training CPI')\nplt.title('STL Decomposition of Australian CPI (Training Data)')\nplt.legend(loc='upper left')\nplt.subplot(412)\nplt.plot(stl_result.trend, label='Trend')\nplt.title('Trend Component')\nplt.legend(loc='upper left')\nplt.subplot(413)\nplt.plot(stl_result.seasonal, label='Seasonal')\nplt.title('Seasonal Component')\nplt.legend(loc='upper left')\nplt.subplot(414)\nplt.plot(stl_result.resid, label='Residual')\nplt.title('Residual Component')\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSTL Decomposition of training CPI data\n\n\n\n\nThe plot shows the STL decomposition of the training CPI data (1995-2023). The training data is decomposed into trend, seasonal, and residual components.\n\nTrend component: The trend component shows the long-term trend of the CPI based on training data. Visually, it is increasing over time.\nSeasonal component: The seasonal component shows the seasonal pattern of the CPI based on training data only.\n\n\n\n\n\n\n\nImportant Note\n\n\n\nHowever, STL decomposition will always create the appearance of seasonal patterns because it’s mathematically designed to extract and redistribute variance into trend, seasonal, and residual components, forcing periodicity even when true seasonality may be weak or nonexistent in the data.\n\n\nWe shouldn’t blindly trust STL’s seasonal component without validation through seasonal plots, which reveal the actual underlying patterns in the raw data and help determine if the detected seasonality is genuine or an algorithmic artifact.\n\n\nCode\n# Seasonal plot: each year as one line (by quarters)\nseasonal_data = pd.Series(stl_result.seasonal.values, index=train_cpi.index)\nyears = seasonal_data.index.year.unique()\n\nplt.figure(figsize=(8, 4))\nfor year in years:\n    year_data = seasonal_data[seasonal_data.index.year == year]\n    # Group by quarters and take mean for each quarter\n    quarterly_data = year_data.groupby(year_data.index.quarter).mean()\n    quarters = range(1, len(quarterly_data) + 1)\n    plt.plot(quarters, quarterly_data.values, label=str(year), alpha=0.7, marker='o', markersize=3)\n\nplt.title('Quarterly Seasonal Plot of Australian CPI (Each Year as One Line)')\nplt.xlabel('Quarter')\nplt.ylabel('Seasonal Component')\nplt.xticks([1, 2, 3, 4], ['Q1\\n(Jan-Mar)', 'Q2\\n(Apr-Jun)', 'Q3\\n(Jul-Sep)', 'Q4\\n(Oct-Dec)'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=3, fontsize=8)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSeasonal Plot of training CPI data\n\n\n\n\nFrom the seasonal plot, we can’t see a clear seasonal pattern. This indicates that the seasonal period is not constant. Later in the modeling process, we can set the seasonal period as zero.\n\nResidual component: displays the residual component from STL decomposition of the CPI time series, which should approximate white noise with a mean near zero and no significant autocorrelation. The residuals mostly fluctuate around zero but show notable spikes around 2009 and 2020, likely corresponding to external shocks, while their overall randomness indicates that the STL decomposition has adequately extracted the systematic trend and seasonal components.\n\n\n\nAutocorrelation Analysis\nThe ACF and PACF are essential diagnostic tools that visualize the internal correlation structure of time series data, helping to identify the correct parameters for forecasting models like ARIMA. Specifically, the ACF helps to detect trends and Moving Average (MA) terms, while the PACF isolates direct relationships to pinpoint the correct Autoregressive (AR) order.\n\n\nCode\n# ACF and PACF plots for training data\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# ACF plot\nplot_acf(train_cpi, lags=24, ax=ax1, alpha=0.05)\nax1.set_title('Autocorrelation Function (ACF) - Australian CPI (Training Data)')\nax1.set_xlabel('Lag (quarters)')\nax1.set_ylabel('Autocorrelation')\nax1.grid(True, alpha=0.3)\n\n# PACF plot\nplot_pacf(train_cpi, lags=16, ax=ax2, alpha=0.05, method='ywm')\nax2.set_title('Partial Autocorrelation Function (PACF) - Australian CPI (Training Data)')\nax2.set_xlabel('Lag (quarters)')\nax2.set_ylabel('Partial Autocorrelation')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nACF and PACF plots for training CPI data\n\n\n\n\nThe ACF’s slow decline shows a clear trend, confirming the data is non-stationary. Meanwhile, the PACF spikes at lag 1 and then cuts off, which points strongly to an AR(1) structure. Together, these signals tell us we need first-order differencing. Although the plots don’t show obvious seasonality.\n\n\nFirst-Order Differencing\n\n\nCode\n# Apply first-order differencing\ncpi_diff = train_cpi.diff().dropna()\n\n# Plot original vs differenced series\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# Original series\nax1.plot(train_cpi.index, train_cpi, color='blue', alpha=0.7)\nax1.set_title('Original CPI Series (Training Data)')\nax1.set_ylabel('CPI Value')\nax1.grid(True, alpha=0.3)\n\n# Differenced series\nax2.plot(cpi_diff.index, cpi_diff, color='red', alpha=0.7)\nax2.set_title('First-Order Differenced CPI Series')\nax2.set_xlabel('Date')\nax2.set_ylabel('CPI Difference')\nax2.grid(True, alpha=0.3)\nax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFirst-order differencing of CPI data\n\n\n\n\nThe top plot shows the original CPI series, while the bottom plot shows the first-order differenced series \\(y_t - y_{t-1}\\). The differenced series appears to be stationary with constant variance.\n\n\n\n\n\n\nImportant Note\n\n\n\nSince the differenced CPI series oscillates around a positive mean rather than zero, the data exhibits a deterministic trend reflecting persistent inflation. Including a ‘drift’ term in the ARIMA model explicitly captures this average growth rate, ensuring that long-term forecasts project a continued upward trajectory rather than erroneously flattening out.\n\n\nNext, we plot the ACF and PACF of the differenced CPI series.\n\n\nCode\n# ACF and PACF of differenced series\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n\n# ACF of differenced data\nplot_acf(cpi_diff, lags=24, ax=ax1, alpha=0.05)\nax1.set_title('ACF of Differenced CPI (d=1)')\nax1.set_xlabel('Lag (quarters)')\nax1.set_ylabel('Autocorrelation')\nax1.grid(True, alpha=0.3)\n\n# PACF of differenced data\nplot_pacf(cpi_diff, lags=16, ax=ax2, alpha=0.05, method='ywm')\nax2.set_title('PACF of Differenced CPI (d=1)')\nax2.set_xlabel('Lag (quarters)')\nax2.set_ylabel('Partial Autocorrelation')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nACF and PACF of differenced CPI data\n\n\n\n\nThe Partial Autocorrelation Function (PACF) exhibits a significant spike at lag 1 followed by a sharp cutoff, while the Autocorrelation Function (ACF) displays a gradual decay. This pattern is characteristic of an Autoregressive process of order 1 (AR(1)), supporting the selection of an ARIMA(1, 1, 0) specification with a drift term to account for the positive mean observed in the differenced series."
  },
  {
    "objectID": "projects/project-1/index.html#methodology",
    "href": "projects/project-1/index.html#methodology",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "Methodology",
    "text": "Methodology\nFor the Exploratory Data Analysis, I include time series visualization, STL decomposition, autocorrelation analysis, and stationarity testing through first-order differencing. ACF and PACF plots guide initial ARIMA(1,1,0) model specification.\nFor the Model Selection, I used an expanding window cross-validation (5 folds) to evaluate ARIMA(p,1,q) combinations where \\(p\\) and \\(q\\) ranges from 0 to 4. The best is the one that minimizes average MSE across validation folds, prioritizing predictive accuracy over information criteria.\nForecast Evaluation compares baseline ARIMA(1,1,0) and optimized models using MAE metrics, with results visualized through comparative plots showing historical data, actual values, and forecasts."
  },
  {
    "objectID": "projects/project-1/index.html#modelling",
    "href": "projects/project-1/index.html#modelling",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "Modelling",
    "text": "Modelling\n\nBaseline Models\n\n\nCode\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Fit ARIMA(1,1,0) model to training data\narima_model = ARIMA(train_cpi, order=(1, 1, 0))\narima_result = arima_model.fit()\n\n# Display model summary\nprint(\"ARIMA(1,1,0) Model Summary:\")\nprint(arima_result.summary())\n\n\nARIMA(1,1,0) Model Summary:\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                    CPI   No. Observations:                  116\nModel:                 ARIMA(1, 1, 0)   Log Likelihood                -113.056\nDate:                Thu, 25 Dec 2025   AIC                            230.112\nTime:                        23:36:40   BIC                            235.601\nSample:                    03-01-1995   HQIC                           232.340\n                         - 12-01-2023                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.6813      0.052     13.070      0.000       0.579       0.783\nsigma2         0.4160      0.031     13.488      0.000       0.356       0.476\n===================================================================================\nLjung-Box (L1) (Q):                  15.29   Jarque-Bera (JB):               365.35\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):               2.13   Skew:                             0.59\nProb(H) (two-sided):                  0.02   Kurtosis:                        11.65\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nThe ARIMA(1,1,0) model was fitted to the training CPI data. The model includes:\n\nAR(1): An autoregressive order of 1 is selected because the Partial Autocorrelation Function (PACF) exhibits a significant spike at the first lag followed by an immediate cutoff.\nI(1): First-order differencing is employed to transform the non-stationary, trending original series into a stationary series with stable variance.\nMA(0): A moving average order of 0 is justified by the Autocorrelation Function (ACF), which displays the gradual geometric decay characteristic of AR processes rather than a sharp drop.\n\n\n\nCode\n# Plot model diagnostics\nfig = arima_result.plot_diagnostics(figsize=(8, 8))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nARIMA(1,1,0) model diagnostics\n\n\n\n\nThe diagnostic plots show:\n\nThe model captures the temporal structure well but exhibits deviations from normality.\nThe Correlogram shows no significant autocorrelation, confirming that the residuals are uncorrelated (white noise).\nHowever, the Normal Q-Q plot and Histogram reveal “heavy tails” (deviations from the red line at the ends), indicating that while the model fits the central data well, it struggles with extreme outliers likely caused by recent economic shocks (e.g., the 2020 volatility).\n\n\n\nExhaustive Search for ARIMA Model\nThis is now the fun part that I normally do. We are going to find the best \\((p, d, q)\\) order via a walk-forward rolling window. The idea here is that we find the best order that minimizes the rolling window’s MSE. What is my motivation? - I don’t trust AIC and BIC alone. Though, AIC estimates the theoretical quality of a model (balancing fit vs. complexity), rolling window MSE measures the practical predictive power by simulating how the model would have actually performed if we had used it in the past.\nWhy use Rolling Window MSE instead of AIC?\n\nReal-World Simulation: It tests the model on data it has never seen, just like in real forecasting.\nDetects Structural Breaks: If a model works well in the 90s but fails in the 2000s, rolling MSE will penalize it. AIC looks at the average fit over the whole history and might miss this instability.\nAvoids Overfitting: AIC can sometimes select complex models that fit “noise.” Cross-validation rarely lies—if the complex model doesn’t predict better, it won’t win.\n\nFirst, we have to talk about: “What is Rolling Window?”- In a nutshell, it is a technique in time series analysis where a calculation or model uses all available historical data up to the current point in time.\n\n\n\nAn illustration of how Rolling Window works.\n\n\nThe Logic:\nThe algorithm works by anchoring the start index at the beginning of the dataset (\\(t=0\\)) and iteratively moving the end index forward.\n\nStep 0: Define a minimum training size (\\(k\\)) and a forecast horizon/test size (\\(h\\)).\nStep 1: Train on the initial window \\([0, k]\\). Forecast \\([k, k+h]\\).\nStep 2: “Expand” the window. Train on \\([0, k+h]\\). Forecast \\([k+h, k+2h]\\).\nStep 3: Expand again. Train on \\([0, k+2h]\\). Forecast \\([k+2h, k+3h]\\).\nRepeat until the test set reaches the end of the total dataset.\n\n\n\n\n\n\n\nThe number of splits matter!!!\n\n\n\nTo account for the significant shift in CPI structure following the onset of the COVID-19 pandemic, we utilized a lower number of splits for cross-validation. Since the growth rate and volatility post-2020 differ drastically from the stable pre-2020 period, validating on earlier years could obscure the model’s ability to adapt to the current regime. Therefore, our validation strategy focuses strictly on the years 2020, 2021, 2022, and 2023 to prioritize recent predictive accuracy. See the change in rate after 2020 in Figure 1\n\n\nBelow, you may find the code to implement expanding window. We do:\n\nWe systematically test every parameter combination of Autoregressive (\\(p\\)) and Moving Average (\\(q\\)) orders ranging from 0 to 4, with differencing (\\(d\\)) fixed at 1 to handle non-stationarity.\nExpanding Window Cross-Validation: We utilize an “anchored” validation strategy where the training set grows larger with each iteration (expanding window), preserving the full historical context while testing on the subsequent 4 quarters (1 year).\nDrift Integration: We employ trend='t' to include a linear trend term, which mathematically translates to a constant “drift” (average inflation rate) in the differenced equation.\nSelection Criterion: The optimal model is identified by minimizing the average Mean Squared Error (MSE) across all 5 validation folds, prioritizing practical predictive accuracy over theoretical fit.\n\n\n\nCode\n# Ensure data is a Series (fixes shape issues)\nif isinstance(train_cpi, pd.DataFrame):\n    train_cpi = train_cpi.squeeze()\n\n# Define parameters\np_values = range(0, 5)\nd_values = [1]\nq_values = range(0, 5)\npdq_combinations = list(product(p_values, d_values, q_values))\n\nbest_score = float(\"inf\")\nbest_cfg = None\n\ntscv = TimeSeriesSplit(n_splits=5, test_size=4)\n\nprint(f\"Starting Grid Search on {len(train_cpi)} data points...\\n\")\n\nfor param in pdq_combinations:\n    mse_scores = []\n    print(f\"Testing ARIMA{param}...\", end=\" \")\n\n    for train_index, test_index in tscv.split(train_cpi):\n        train_fold = train_cpi.iloc[train_index]\n        test_fold = train_cpi.iloc[test_index]\n\n        try:\n            # --- THE FIX IS HERE ---\n            # Use trend='t' when d=1 to add a \"Drift\" term\n            model = ARIMA(train_fold, order=param, trend='t', enforce_stationarity=False)\n            model_fit = model.fit()\n\n            forecast = model_fit.forecast(steps=len(test_fold))\n            mse = mean_squared_error(test_fold, forecast)\n            mse_scores.append(mse)\n\n        except Exception as e:\n            continue\n\n    if len(mse_scores) &gt; 0:\n        avg_mse = np.mean(mse_scores)\n        print(f\"-&gt; Avg MSE: {avg_mse:.4f}\")\n\n        if avg_mse &lt; best_score:\n            best_score = avg_mse\n            best_cfg = param\n    else:\n        print(\"-&gt; Failed to converge.\")\n\nprint(\"-\" * 40)\nprint(f\"Best Model: ARIMA{best_cfg} with Rolling MSE: {best_score:.4f}\")\n\n\nStarting Grid Search on 116 data points...\n\nTesting ARIMA(0, 1, 0)... -&gt; Avg MSE: 7.0201\nTesting ARIMA(0, 1, 1)... -&gt; Avg MSE: 6.7206\nTesting ARIMA(0, 1, 2)... -&gt; Avg MSE: 6.5812\nTesting ARIMA(0, 1, 3)... -&gt; Avg MSE: 6.4779\nTesting ARIMA(0, 1, 4)... -&gt; Avg MSE: 6.2234\nTesting ARIMA(1, 1, 0)... -&gt; Avg MSE: 6.4275\nTesting ARIMA(1, 1, 1)... -&gt; Avg MSE: 6.0812\nTesting ARIMA(1, 1, 2)... -&gt; Avg MSE: 6.4024\nTesting ARIMA(1, 1, 3)... -&gt; Avg MSE: 6.3380\nTesting ARIMA(1, 1, 4)... -&gt; Avg MSE: 6.8053\nTesting ARIMA(2, 1, 0)... -&gt; Avg MSE: 6.2800\nTesting ARIMA(2, 1, 1)... -&gt; Avg MSE: 5.7589\nTesting ARIMA(2, 1, 2)... -&gt; Avg MSE: 6.7963\nTesting ARIMA(2, 1, 3)... -&gt; Avg MSE: 7.0020\nTesting ARIMA(2, 1, 4)... -&gt; Avg MSE: 7.2795\nTesting ARIMA(3, 1, 0)... -&gt; Avg MSE: 6.1979\nTesting ARIMA(3, 1, 1)... -&gt; Avg MSE: 6.3408\nTesting ARIMA(3, 1, 2)... -&gt; Avg MSE: 6.8001\nTesting ARIMA(3, 1, 3)... -&gt; Avg MSE: 7.3985\nTesting ARIMA(3, 1, 4)... -&gt; Avg MSE: 7.2995\nTesting ARIMA(4, 1, 0)... -&gt; Avg MSE: 6.0851\nTesting ARIMA(4, 1, 1)... -&gt; Avg MSE: 5.7986\nTesting ARIMA(4, 1, 2)... -&gt; Avg MSE: 5.9326\nTesting ARIMA(4, 1, 3)... -&gt; Avg MSE: 6.6206\nTesting ARIMA(4, 1, 4)... -&gt; Avg MSE: 6.8201\n----------------------------------------\nBest Model: ARIMA(2, 1, 1) with Rolling MSE: 5.7589\n\n\nNow, let’s examine the diagnostic plots for our best model ARIMA(2,1,1) to assess its adequacy.\n\n\nCode\n# Fit the best model ARIMA(2,1,1) on the training data\nbest_model = ARIMA(train_cpi, order=best_cfg, trend='t', enforce_stationarity=False)\nbest_model_fit = best_model.fit()\n\n# Plot model diagnostics\nfig = best_model_fit.plot_diagnostics(figsize=(8, 8))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nARIMA(2,1,1) best model diagnostics\n\n\n\n\nThe diagnostic plots for the best ARIMA(2,1,1) model show:\n\nResidual Independence: The Correlogram shows no significant autocorrelation, as the lags remain well within the significance bounds. This confirms that the residuals are uncorrelated (white noise), indicating the model has successfully captured the temporal structure of the series.\nDeviations from Normality: The Normal Q-Q plot and Histogram exhibit distinct “heavy tails,” evidenced by significant deviations from the red reference line at the extremes. This suggests that while the model fits the central data well, it struggles to capture extreme outliers, likely resulting from the exogenous economic shocks observed around 2020.\n\n\n\nA note on model comparison.\nNow, we wonder how does this best model ARIMA(2,1,1)compare to the baseline model ARIMA(1,1,0). The baseline model yields an average MSE of 6.4275 while the best grid-search is 5.5281. Which in my view is much better, now I can justify that- “Hey, my model choice is based on unseen data which is a solid justification for choosing the best model for predictive purposes.”"
  },
  {
    "objectID": "projects/project-1/index.html#predictions",
    "href": "projects/project-1/index.html#predictions",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "Predictions",
    "text": "Predictions\nNow let’s compare the forecasting performance of both models by generating predictions for the next 4 quarters (March 2024 - December 2024) and plotting them against the actual values.\n\n\nCode\n# Extract test CPI series\ntest_cpi = test_df.set_index('Date')['CPI']\n\n# Fit both models on the training data\nbaseline_model = ARIMA(train_cpi, order=(1,1,0), trend='t', enforce_stationarity=False)\nbaseline_fit = baseline_model.fit()\n\nbest_model = ARIMA(train_cpi, order=best_cfg, trend='t', enforce_stationarity=False)\nbest_fit = best_model.fit()\n\n# Generate forecasts for the test period (4 quarters)\nn_periods = len(test_cpi)\nbaseline_forecast = baseline_fit.forecast(steps=n_periods)\nbest_forecast = best_fit.forecast(steps=n_periods)\n\n# Calculate forecast errors\nbaseline_mse = mean_squared_error(test_cpi, baseline_forecast)\nbest_mse = mean_squared_error(test_cpi, best_forecast)\n\n# Plot the results\nplt.figure(figsize=(8, 6))\n\n# Filter training data to show only from 2018 onwards for clearer visualization\ntrain_cpi_filtered = train_cpi[train_cpi.index &gt;= '2020-01-01']\n\n# Plot historical data (2018 onwards)\nplt.plot(train_cpi_filtered.index, train_cpi_filtered, color='black', label='Training Data (2018+)', linewidth=2)\n\n# Plot actual test data\nplt.plot(test_cpi.index, test_cpi, color='blue', label='Actual Test Data', linewidth=2, marker='o')\n\n# Plot forecasts as points instead of lines\nplt.scatter(test_cpi.index, baseline_forecast, color='red', marker='x', s=25, linewidth=3, label=f'Baseline ARIMA(1,1,0) Forecast (MSE: {baseline_mse:.2f})', zorder=5)\nplt.scatter(test_cpi.index, best_forecast, color='green', marker='o', s=25, linewidth=3, label=f'Best ARIMA{best_cfg} Forecast (MSE: {best_mse:.2f})', zorder=5)\n\n# Add vertical line to separate train/test\nplt.axvline(x=train_cpi.index[-1], color='gray', linestyle=':', alpha=0.7, label='Train/Test Split')\n\n# Formatting\nplt.title('CPI Forecast Comparison: Baseline vs Best Model', fontsize=14, fontweight='bold')\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('CPI Value', fontsize=12)\nplt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.35), ncol=2)\nplt.grid(False)\nax_current = plt.gca()\nsns.despine(ax=ax_current, top=True, right=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nForecast comparison: Baseline ARIMA(1,1,0) vs Best ARIMA(2,1,1)\n\n\n\n\nThe forecast comparison plot shows:\n\nTraining Data: The historical CPI data used to train both models (shown in black)\nActual Test Data: The real CPI values for Q1-Q4 2024 (shown in blue with markers)\nBaseline Forecast: ARIMA(1,1,0) predictions (red dashed line)\nBest Model Forecast: ARIMA(2,1,1) predictions (green dashed line)\n\nThe plot clearly demonstrates how the best model provides more accurate predictions compared to the baseline, particularly in capturing the recent trends in CPI data."
  },
  {
    "objectID": "projects/project-1/index.html#results",
    "href": "projects/project-1/index.html#results",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "Results",
    "text": "Results\n\n\nCode\n# Create a summary table of forecast results\nforecast_dates = test_cpi.index.strftime('%Y-%m-%d')\nquarters = ['Q1', 'Q2', 'Q3', 'Q4']\n\nforecast_comparison_df = pd.DataFrame({\n    'Actual CPI': test_cpi.values,\n    'Baseline Forecast': baseline_forecast,\n    'Best Model Forecast': best_forecast,\n    'Baseline Error': np.abs(test_cpi.values - baseline_forecast),\n    'Best Model Error': np.abs(test_cpi.values - best_forecast)\n})\n\n# Display the forecast comparison table\nforecast_comparison_df.round(2)\n\n\n\n\n\n\n\n\n\nActual CPI\nBaseline Forecast\nBest Model Forecast\nBaseline Error\nBest Model Error\n\n\n\n\n2024-03-01\n137.4\n136.79\n137.15\n0.61\n0.25\n\n\n2024-06-01\n138.8\n137.44\n138.14\n1.36\n0.66\n\n\n2024-09-01\n139.1\n138.07\n139.07\n1.03\n0.03\n\n\n2024-12-01\n139.4\n138.70\n139.96\n0.70\n0.56\n\n\n\n\n\n\n\nAfter all, it is clear that the exhaustive-search approach performs better in terms of predictive accuracy. In this project, I want to demonstrate that using the ACF and the PACF plots to indentify the MA and AR terms is really good as a bench mark, however, the exhaustive search ensures that we have explored all the possible models and by choosing the best \\((p,d,q)\\) order that minimizes the MSE, we can arrive as a model that predicts CPI well."
  },
  {
    "objectID": "projects/project-1/index.html#key-take-aways.",
    "href": "projects/project-1/index.html#key-take-aways.",
    "title": "Exhaustive ARIMA Modeling for CPI Forecasting",
    "section": "Key Take-aways.",
    "text": "Key Take-aways.\nThe most significant challenge in this project was implementing the exhaustive search Cross-Validation for the time series data. To manage the complexity, I stepped back and adopted a modular approach:\n\nIndex Verification: First, I debugged the rolling window to ensure the correct training and testing indices were used at every iteration.\nParameter Grid: Next, I verified that the \\((p,d,q)\\) combinations were generating correctly.\nModel Integration: Finally, I integrated the ARIMA fitting process into the loop.\n\nThe rigorous testing paid off, and seeing the code work successfully was incredibly satisfying. I have developed a strong appreciation for this exhaustive search methodology. While computational limits exist today, I look forward to the day quantum computing makes it possible to explore even vaster parameter spaces for my future projects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Khanh N. (Alex) Bui",
    "section": "",
    "text": "Khanh N. (Alex) Bui\n\nTime Series | Machine Learning | Portfolio Optimization\nI’m a Data Science major at The University of Sydney, currently in my third year and considering pursuing Honours in Data Science. My passion lies in time series analysis, machine learning, and portfolio optimization. Read more about me here.\nFeel free to explore my projects and blog posts to see what I’ve been working on!\n\nProjects Blog Posts Resume"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "From Fairways to Data Science\n\n\n\n\n\n\nMy Journey\n\n\n\nHow I went from junior golf to data science.\n\n\n\n\n\nNov 15, 2025\n\n\n\n\n\n\nNo matching items"
  }
]